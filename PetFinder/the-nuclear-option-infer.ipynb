{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6816c1f2",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:11:18.644709Z",
     "iopub.status.busy": "2022-01-10T23:11:18.642448Z",
     "iopub.status.idle": "2022-01-10T23:11:18.725909Z",
     "shell.execute_reply": "2022-01-10T23:11:18.728232Z",
     "shell.execute_reply.started": "2022-01-10T13:11:34.471269Z"
    },
    "papermill": {
     "duration": 0.137656,
     "end_time": "2022-01-10T23:11:18.728836",
     "exception": false,
     "start_time": "2022-01-10T23:11:18.591180",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\">\n",
       "        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Raleway\">\n",
       "        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Oswald\">\n",
       "        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Open Sans\">\n",
       "        <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n",
       "        <style>\n",
       "        .title-section{\n",
       "            font-family: \"Oswald\", Arial, sans-serif;\n",
       "            font-weight: bold;\n",
       "            color: \"#6A8CAF\";\n",
       "            letter-spacing: 6px;\n",
       "        }\n",
       "        hr { border: 1px solid #E58F65 !important;\n",
       "             color: #E58F65 !important;\n",
       "             background: #E58F65 !important;\n",
       "           }\n",
       "        body {\n",
       "            font-family: \"Open Sans\", sans-serif;\n",
       "            }        \n",
       "        </style>\n",
       "    </head>    \n",
       "</html>\n",
       "<style>\n",
       "div #notebook {\n",
       "background-color: white;\n",
       "font-family: 'Open Sans', Helvetica, sans-serif;\n",
       "line-height: 20px;\n",
       "}\n",
       "\n",
       "#notebook-container {\n",
       "margin-top: 2em;\n",
       "padding-top: 2em;\n",
       "border-top: 4px solid #E58F65; /* light orange */\n",
       "-webkit-box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
       "    box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
       "}\n",
       "\n",
       "div .input {\n",
       "margin-bottom: 1em;\n",
       "}\n",
       "\n",
       ".rendered_html h1, .rendered_html h2, .rendered_html h3, .rendered_html h4, .rendered_html h5, .rendered_html h6 {\n",
       "color: #E58F65; /* light orange */\n",
       "font-weight: 600;\n",
       "}\n",
       "\n",
       ".rendered_html code {\n",
       "    background-color: #efefef; /* light gray */\n",
       "}\n",
       "\n",
       ".CodeMirror {\n",
       "color: #8c8c8c; /* dark gray */\n",
       "padding: 0.7em;\n",
       "}\n",
       "\n",
       "div.input_area {\n",
       "border: none;\n",
       "    background-color: rgba(229, 143, 101, 0.1); /* rgba(229, 143, 101, 0.1); light orange [exactly #E58F65] */\n",
       "    border-top: 2px solid #E58F65; /* light orange */\n",
       "}\n",
       "\n",
       "div.input_prompt {\n",
       "color: #1DBCCD; /* light blue */\n",
       "}\n",
       "\n",
       "div.output_prompt {\n",
       "color: #EB9514; /* strong orange */\n",
       "}\n",
       "\n",
       "div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before {\n",
       "background: #E58F65; /* light orange */\n",
       "}\n",
       "\n",
       "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
       "    border-color: #E58F65; /* light orange */\n",
       "}\n",
       "\n",
       ".edit_mode div.cell.selected:before {\n",
       "background: #E58F65; /* light orange */\n",
       "}\n",
       "\n",
       ".edit_mode div.cell.selected {\n",
       "border-color: #E58F65; /* light orange */\n",
       "\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if os.path.exists('/t/Datasets_local/kaggle_paw/internal'): os.chdir('/t/Datasets_local/kaggle_paw/internal')\n",
    "elif os.path.exists('/t/Datasets/kaggle_paw/internal'): os.chdir('/t/Datasets/kaggle_paw/internal')\n",
    "from IPython.core.display import display, HTML, Javascript\n",
    "\n",
    "# ----- Notebook Theme -----\n",
    "\n",
    "notebook_theme = 'carrot'\n",
    "color_maps = {'turquoise': ['#1abc9c', '#e8f8f5', '#d1f2eb', '#a3e4d7', '#76d7c4', '#48c9b0', '#1abc9c', '#17a589', '#148f77', '#117864', '#0e6251'], 'green': ['#16a085', '#e8f6f3', '#d0ece7', '#a2d9ce', '#73c6b6', '#45b39d', '#16a085', '#138d75', '#117a65', '#0e6655', '#0b5345'], 'emerald': ['#2ecc71', '#eafaf1', '#d5f5e3', '#abebc6', '#82e0aa', '#58d68d', '#2ecc71', '#28b463', '#239b56', '#1d8348', '#186a3b'], 'nephritis': ['#27ae60', '#e9f7ef', '#d4efdf', '#a9dfbf', '#7dcea0', '#52be80', '#27ae60', '#229954', '#1e8449', '#196f3d', '#145a32'], 'peter': ['#3498db', '#ebf5fb', '#d6eaf8', '#aed6f1', '#85c1e9', '#5dade2', '#3498db', '#2e86c1', '#2874a6', '#21618c', '#1b4f72'], 'belize': ['#2980b9', '#eaf2f8', '#d4e6f1', '#a9cce3', '#7fb3d5', '#5499c7', '#2980b9', '#2471a3', '#1f618d', '#1a5276', '#154360'], 'amethyst': ['#9b59b6', '#f5eef8', '#ebdef0', '#d7bde2', '#c39bd3', '#af7ac5', '#9b59b6', '#884ea0', '#76448a', '#633974', '#512e5f'], 'wisteria': ['#8e44ad', '#f4ecf7', '#e8daef', '#d2b4de', '#bb8fce', '#a569bd', '#8e44ad', '#7d3c98', '#6c3483', '#5b2c6f', '#4a235a'], 'wet': ['#34495e', '#ebedef', '#d6dbdf', '#aeb6bf', '#85929e', '#5d6d7e', '#34495e', '#2e4053', '#283747', '#212f3c', '#1b2631'], 'midnight': ['#2c3e50', '#eaecee', '#d5d8dc', '#abb2b9', '#808b96', '#566573', '#2c3e50', '#273746', '#212f3d', '#1c2833', '#17202a'], 'sunflower': ['#f1c40f', '#fef9e7', '#fcf3cf', '#f9e79f', '#f7dc6f', '#f4d03f', '#f1c40f', '#d4ac0d', '#b7950b', '#9a7d0a', '#7d6608'], 'orange': ['#f39c12', '#fef5e7', '#fdebd0', '#fad7a0', '#f8c471', '#f5b041', '#f39c12', '#d68910', '#b9770e', '#9c640c', '#7e5109'], 'carrot': ['#e67e22', '#fdf2e9', '#fae5d3', '#f5cba7', '#f0b27a', '#eb984e', '#e67e22', '#ca6f1e', '#af601a', '#935116', '#784212'], 'pumpkin': ['#d35400', '#fbeee6', '#f6ddcc', '#edbb99', '#e59866', '#dc7633', '#d35400', '#ba4a00', '#a04000', '#873600', '#6e2c00'], 'alizarin': ['#e74c3c', '#fdedec', '#fadbd8', '#f5b7b1', '#f1948a', '#ec7063', '#e74c3c', '#cb4335', '#b03a2e', '#943126', '#78281f'], 'pomegranate': ['#c0392b', '#f9ebea', '#f2d7d5', '#e6b0aa', '#d98880', '#cd6155', '#c0392b', '#a93226', '#922b21', '#7b241c', '#641e16'], 'clouds': ['#ecf0f1', '#fdfefe', '#fbfcfc', '#f7f9f9', '#f4f6f7', '#f0f3f4', '#ecf0f1', '#d0d3d4', '#b3b6b7', '#979a9a', '#7b7d7d'], 'silver': ['#bdc3c7', '#f8f9f9', '#f2f3f4', '#e5e7e9', '#d7dbdd', '#cacfd2', '#bdc3c7', '#a6acaf', '#909497', '#797d7f', '#626567'], 'concrete': ['#95a5a6', '#f4f6f6', '#eaeded', '#d5dbdb', '#bfc9ca', '#aab7b8', '#95a5a6', '#839192', '#717d7e', '#5f6a6a', '#4d5656'], 'asbestos': ['#7f8c8d', '#f2f4f4', '#e5e8e8', '#ccd1d1', '#b2babb', '#99a3a4', '#7f8c8d', '#707b7c', '#616a6b', '#515a5a', '#424949']}\n",
    "# color_maps = {'red': ['#f44336', '#ffebee', '#ffcdd2', '#ef9a9a', '#e57373', '#ef5350', '#f44336', '#e53935', '#d32f2f', '#c62828', '#b71c1c', '#ff8a80', '#ff5252', '#ff1744', '#d50000'], 'pink': ['#e91e63', '#fce4ec', '#f8bbd0', '#f48fb1', '#f06292', '#ec407a', '#e91e63', '#d81b60', '#c2185b', '#ad1457', '#880e4f', '#ff80ab', '#ff4081', '#f50057', '#c51162'], 'purple': ['#9c27b0', '#f3e5f5', '#e1bee7', '#ce93d8', '#ba68c8', '#ab47bc', '#9c27b0', '#8e24aa', '#7b1fa2', '#6a1b9a', '#4a148c', '#ea80fc', '#e040fb', '#d500f9', '#aa00ff'], 'deep': ['#673ab7', '#ede7f6', '#d1c4e9', '#b39ddb', '#9575cd', '#7e57c2', '#673ab7', '#5e35b1', '#512da8', '#4527a0', '#311b92', '#b388ff', '#7c4dff', '#651fff', '#6200ea', '#ff5722', '#fbe9e7', '#ffccbc', '#ffab91', '#ff8a65', '#ff7043', '#ff5722', '#f4511e', '#e64a19', '#d84315', '#bf360c', '#ff9e80', '#ff6e40', '#ff3d00', '#dd2c00'], 'indigo': ['#3f51b5', '#e8eaf6', '#c5cae9', '#9fa8da', '#7986cb', '#5c6bc0', '#3f51b5', '#3949ab', '#303f9f', '#283593', '#1a237e', '#8c9eff', '#536dfe', '#3d5afe', '#304ffe'], 'blue': ['#2196f3', '#e3f2fd', '#bbdefb', '#90caf9', '#64b5f6', '#42a5f5', '#2196f3', '#1e88e5', '#1976d2', '#1565c0', '#0d47a1', '#82b1ff', '#448aff', '#2979ff', '#2962ff', '#607d8b', '#eceff1', '#cfd8dc', '#b0bec5', '#90a4ae', '#78909c', '#607d8b', '#546e7a', '#455a64', '#37474f', '#263238'], 'light': ['#03a9f4', '#e1f5fe', '#b3e5fc', '#81d4fa', '#4fc3f7', '#29b6f6', '#03a9f4', '#039be5', '#0288d1', '#0277bd', '#01579b', '#80d8ff', '#40c4ff', '#00b0ff', '#0091ea', '#8bc34a', '#f1f8e9', '#dcedc8', '#c5e1a5', '#aed581', '#9ccc65', '#8bc34a', '#7cb342', '#689f38', '#558b2f', '#33691e', '#ccff90', '#b2ff59', '#76ff03', '#64dd17'], 'cyan': ['#00bcd4', '#e0f7fa', '#b2ebf2', '#80deea', '#4dd0e1', '#26c6da', '#00bcd4', '#00acc1', '#0097a7', '#00838f', '#006064', '#84ffff', '#18ffff', '#00e5ff', '#00b8d4'], 'teal': ['#009688', '#e0f2f1', '#b2dfdb', '#80cbc4', '#4db6ac', '#26a69a', '#009688', '#00897b', '#00796b', '#00695c', '#004d40', '#a7ffeb', '#64ffda', '#1de9b6', '#00bfa5'], 'green': ['#4caf50', '#e8f5e9', '#c8e6c9', '#a5d6a7', '#81c784', '#66bb6a', '#4caf50', '#43a047', '#388e3c', '#2e7d32', '#1b5e20', '#b9f6ca', '#69f0ae', '#00e676', '#00c853'], 'lime': ['#cddc39', '#f9fbe7', '#f0f4c3', '#e6ee9c', '#dce775', '#d4e157', '#cddc39', '#c0ca33', '#afb42b', '#9e9d24', '#827717', '#f4ff81', '#eeff41', '#c6ff00', '#aeea00'], 'yellow': ['#ffeb3b', '#fffde7', '#fff9c4', '#fff59d', '#fff176', '#ffee58', '#ffeb3b', '#fdd835', '#fbc02d', '#f9a825', '#f57f17', '#ffff8d', '#ffff00', '#ffea00', '#ffd600'], 'amber': ['#ffc107', '#fff8e1', '#ffecb3', '#ffe082', '#ffd54f', '#ffca28', '#ffc107', '#ffb300', '#ffa000', '#ff8f00', '#ff6f00', '#ffe57f', '#ffd740', '#ffc400', '#ffab00'], 'orange': ['#ff9800', '#fff3e0', '#ffe0b2', '#ffcc80', '#ffb74d', '#ffa726', '#ff9800', '#fb8c00', '#f57c00', '#ef6c00', '#e65100', '#ffd180', '#ffab40', '#ff9100', '#ff6d00'], 'brown': ['#795548', '#efebe9', '#d7ccc8', '#bcaaa4', '#a1887f', '#8d6e63', '#795548', '#6d4c41', '#5d4037', '#4e342e', '#3e2723'], 'grey': ['#9e9e9e', '#fafafa', '#f5f5f5', '#eeeeee', '#e0e0e0', '#bdbdbd', '#9e9e9e', '#757575', '#616161', '#424242', '#212121'], 'white': ['#ffffff'], 'black': ['#000000']}\n",
    "\n",
    "color_maps = {i: color_maps[i] for i in color_maps if i not in ['clouds', 'silver', 'concrete', 'asbestos', 'wet asphalt', 'midnight blue', 'wet']}\n",
    "\n",
    "CMAP = 'Oranges'\n",
    "prompt = '#1DBCCD'\n",
    "main_color = '#E58F65' # color_maps[notebook_theme]\n",
    "strong_main_color = '#EB9514' # = color_maps[notebook_theme] \n",
    "custom_colors = [strong_main_color, main_color]\n",
    "\n",
    "# ----- Notebook Theme -----\n",
    "\n",
    "html_contents =\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "    <head>\n",
    "        <link rel=\"stylesheet\" href=\"https://www.w3schools.com/w3css/4/w3.css\">\n",
    "        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Raleway\">\n",
    "        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Oswald\">\n",
    "        <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Open Sans\">\n",
    "        <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css\">\n",
    "        <style>\n",
    "        .title-section{\n",
    "            font-family: \"Oswald\", Arial, sans-serif;\n",
    "            font-weight: bold;\n",
    "            color: \"#6A8CAF\";\n",
    "            letter-spacing: 6px;\n",
    "        }\n",
    "        hr { border: 1px solid #E58F65 !important;\n",
    "             color: #E58F65 !important;\n",
    "             background: #E58F65 !important;\n",
    "           }\n",
    "        body {\n",
    "            font-family: \"Open Sans\", sans-serif;\n",
    "            }        \n",
    "        </style>\n",
    "    </head>    \n",
    "</html>\n",
    "\"\"\"\n",
    "htm1 = (html_contents)\n",
    "css_file = '''\n",
    "div #notebook {\n",
    "background-color: white;\n",
    "font-family: 'Open Sans', Helvetica, sans-serif;\n",
    "line-height: 20px;\n",
    "}\n",
    "\n",
    "#notebook-container {\n",
    "margin-top: 2em;\n",
    "padding-top: 2em;\n",
    "border-top: 4px solid %s; /* light orange */\n",
    "-webkit-box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
    "    box-shadow: 0px 0px 8px 2px rgba(224, 212, 226, 0.5); /* pink */\n",
    "}\n",
    "\n",
    "div .input {\n",
    "margin-bottom: 1em;\n",
    "}\n",
    "\n",
    ".rendered_html h1, .rendered_html h2, .rendered_html h3, .rendered_html h4, .rendered_html h5, .rendered_html h6 {\n",
    "color: %s; /* light orange */\n",
    "font-weight: 600;\n",
    "}\n",
    "\n",
    ".rendered_html code {\n",
    "    background-color: #efefef; /* light gray */\n",
    "}\n",
    "\n",
    ".CodeMirror {\n",
    "color: #8c8c8c; /* dark gray */\n",
    "padding: 0.7em;\n",
    "}\n",
    "\n",
    "div.input_area {\n",
    "border: none;\n",
    "    background-color: %s; /* rgba(229, 143, 101, 0.1); light orange [exactly #E58F65] */\n",
    "    border-top: 2px solid %s; /* light orange */\n",
    "}\n",
    "\n",
    "div.input_prompt {\n",
    "color: %s; /* light blue */\n",
    "}\n",
    "\n",
    "div.output_prompt {\n",
    "color: %s; /* strong orange */\n",
    "}\n",
    "\n",
    "div.cell.selected:before, div.cell.selected.jupyter-soft-selected:before {\n",
    "background: %s; /* light orange */\n",
    "}\n",
    "\n",
    "div.cell.selected, div.cell.selected.jupyter-soft-selected {\n",
    "    border-color: %s; /* light orange */\n",
    "}\n",
    "\n",
    ".edit_mode div.cell.selected:before {\n",
    "background: %s; /* light orange */\n",
    "}\n",
    "\n",
    ".edit_mode div.cell.selected {\n",
    "border-color: %s; /* light orange */\n",
    "\n",
    "}\n",
    "'''\n",
    "def to_rgb(h): return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "main_color_rgba = 'rgba(%s, %s, %s, 0.1)' % (to_rgb(main_color[1:])[0], to_rgb(main_color[1:])[1], to_rgb(main_color[1:])[2])\n",
    "open('notebook.css', 'w').write(css_file % (main_color, main_color, main_color_rgba, main_color,  prompt, strong_main_color, main_color, main_color, main_color, main_color))\n",
    "from IPython.core.display import display, HTML, Javascript\n",
    "def nb(): return (\"<style>\" + open(\"notebook.css\", \"r\").read() + \"</style>\")\n",
    "htm2 = nb()\n",
    "htm3 = htm1 + htm2\n",
    "import os, sys\n",
    "if not os.path.exists('../input'): os.chdir('/t/Datasets/kaggle_tps_nov/internal/')\n",
    "HTML(htm3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faad32c5",
   "metadata": {
    "_cell_guid": "89966624-09a9-4837-ba7c-6e59bb02704a",
    "_uuid": "cc975077-f0ae-43c8-bd57-719b9a0a7392",
    "papermill": {
     "duration": 0.093826,
     "end_time": "2022-01-10T23:11:18.911899",
     "exception": false,
     "start_time": "2022-01-10T23:11:18.818073",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaae328",
   "metadata": {
    "_cell_guid": "76c69bf9-95e3-43ac-9c76-122179beda05",
    "_uuid": "13a87c0b-f21a-4b80-b77e-8ea068c1c62c",
    "papermill": {
     "duration": 0.087769,
     "end_time": "2022-01-10T23:11:19.099547",
     "exception": false,
     "start_time": "2022-01-10T23:11:19.011778",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div>    \n",
    "    <div style = \"float:left; width:55%; overflow:hidden;\">        \n",
    "        <br><br><br><br>        \n",
    "        <span style = \"float:right;\">\n",
    "        <h2>Nuclear Protocols for Image Classification</h2>\n",
    "        <p><b>Tutorial: How to destroy image classification datasets? ü§î</b><br><br>            \n",
    "            <b>Models:</b><br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;EfficinetNet<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;NFNet<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;ViT<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;Swin Transformer<br> \n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;DOLG<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;Hybrid Swin Transformer<br>\n",
    "            &nbsp;&nbsp;&nbsp;&nbsp;External Attention Transformer<br>\n",
    "            </p>\n",
    "        <br>\n",
    "        <b></b>\n",
    "        <b>\n",
    "        - üåé <a href=\"https://www.kaggle.com/c/petfinder-pawpularity-score/discussion/295399\">Discussion Thread</a>\n",
    "        <br>\n",
    "        - üá∞ <a href=\"https://www.kaggle.com/c/petfinder-pawpularity-score\">The Competition</a>\n",
    "        </b>            \n",
    "        </span>\n",
    "    </div>\n",
    "    <div style=\"float:right; width:35%; max-height:300px; overflow: hidden;\">        \n",
    "        <img src=\"https://i.ibb.co/gDKWJZR/gif-logo.gif\" style = \"max-height: 300px;\">         \n",
    "    </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec8d59",
   "metadata": {
    "_cell_guid": "0fff39f7-3a65-4286-8efd-dd23ce2e1de6",
    "_uuid": "9edd7d0d-294b-473d-96fb-c156c9feb2c7",
    "papermill": {
     "duration": 0.090098,
     "end_time": "2022-01-10T23:11:19.279589",
     "exception": false,
     "start_time": "2022-01-10T23:11:19.189491",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "## Training Notebook [Here](https://www.kaggle.com/yamqwe/tf-nfnet-vit-efn-tta-train) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbda40af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-01T13:40:33.004611Z",
     "iopub.status.busy": "2021-12-01T13:40:33.004216Z",
     "iopub.status.idle": "2021-12-01T13:40:33.03104Z",
     "shell.execute_reply": "2021-12-01T13:40:33.029818Z",
     "shell.execute_reply.started": "2021-12-01T13:40:33.004509Z"
    },
    "papermill": {
     "duration": 0.057371,
     "end_time": "2022-01-10T23:11:19.435935",
     "exception": false,
     "start_time": "2022-01-10T23:11:19.378564",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<hr>\n",
    "<center><br><b><h1>The Nuclear Option</h1></b> <br>Only to be considered once we have exhausted every other option<br><b>The Pipeline:</b> The proposed method is an ensemle of 7 stacking piplines (140+ models).<br>Each pipeline has a 2nd stage model that is trained on the extracted image embedding [Figure[1]].</center>\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/McJ39mW/image-nuke.png\">\n",
    "\n",
    "<br>\n",
    "<center>Figure[1]: Nuclear protocols for image classification: expected results</center>\n",
    "    \n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44bc1c0",
   "metadata": {
    "_cell_guid": "d70815a1-3398-43b2-be85-2230fffd89a9",
    "_uuid": "e9b980c5-036c-4e25-992b-115169a95ba4",
    "papermill": {
     "duration": 0.053911,
     "end_time": "2022-01-10T23:11:19.545059",
     "exception": false,
     "start_time": "2022-01-10T23:11:19.491148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### **Nuclear Protocols for Image Classification**\n",
    "\n",
    "#### Table Of Content\n",
    "\n",
    "1. [üìë Table Of Content](#outline) \n",
    "\n",
    "2. [üìö Imports](#imports) \n",
    "\n",
    "3. [‚öôÔ∏è Configuration](#config)\n",
    "\n",
    "4. [‚öôÔ∏è TPU Configuration](#tpu_config)\n",
    "\n",
    "5. [üóÇÔ∏è Loading Meta Data](#loading)\n",
    "\n",
    "6. [üé∞ CV: Stratified KFold for Regression](#cv)\n",
    "\n",
    "7. [‚úÇÔ∏è Augmentations](#augmentations)\n",
    "\n",
    "8. [üîß Data Pipeline](#data_pipeline)\n",
    "\n",
    "9. [üöÄ The Models](#the_models)\n",
    "\n",
    "    9.1. [‚ò¢Ô∏è Vision Transformer (ViT)](#vit)\n",
    "    \n",
    "    9.2. [‚ò¢Ô∏è Normalization Free Network (NFNet) ](#nfnet)\n",
    "    \n",
    "    9.3. [‚ò¢Ô∏è Efficientnet B7](#efficientnet)\n",
    "    \n",
    "    9.4. [‚ò¢Ô∏è Hybrid Swin Transformer](#hybrid_swin)\n",
    "    \n",
    "    9.5. [‚ò¢Ô∏è DOLG](#dolg)  \n",
    "    \n",
    "    9.6. [‚ò¢Ô∏è Swin Transformer](#swin)\n",
    "    \n",
    "    9.7. [‚ò¢Ô∏è External Attention Transformer (EAT)](#eat)\n",
    "        \n",
    "\n",
    "9. [üî• 1st Stage Inference](#first_infer)\n",
    "\n",
    "10. [üá∞ Submission to Kaggle (1st stage)](#first_submit)\n",
    "\n",
    "11. [üî• 2nd Stage Inference (Catboost)](#catboost)\n",
    "\n",
    "12. [üá∞ Ensemble Submission to Kaggle](#ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba859a",
   "metadata": {
    "papermill": {
     "duration": 0.049413,
     "end_time": "2022-01-10T23:11:19.646924",
     "exception": false,
     "start_time": "2022-01-10T23:11:19.597511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Installations & Environment Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2264ed8a",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:11:19.757547Z",
     "iopub.status.busy": "2022-01-10T23:11:19.756484Z",
     "iopub.status.idle": "2022-01-10T23:12:50.270631Z",
     "shell.execute_reply": "2022-01-10T23:12:50.269918Z",
     "shell.execute_reply.started": "2022-01-10T13:11:34.529356Z"
    },
    "papermill": {
     "duration": 90.574387,
     "end_time": "2022-01-10T23:12:50.270785",
     "exception": false,
     "start_time": "2022-01-10T23:11:19.696398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./efficientnet-keras-dataset/efficientnet_kaggle\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.1) (0.18.3)\r\n",
      "Requirement already satisfied: numpy>=1.16.5 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.19.5)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.9.0)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.6.3)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.7.3)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (8.2.0)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (3.5.1)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.2.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2021.11.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (3.0.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.8.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.3.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (4.28.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (21.3)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (0.11.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.16.0)\r\n",
      "Building wheels for collected packages: efficientnet\r\n",
      "  Building wheel for efficientnet (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for efficientnet: filename=efficientnet-1.1.1-py3-none-any.whl size=20061 sha256=8cd5b252f114d9679477776d9ed7adafa04cb4dab54a3ed372f65f73a27455cd\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-hqn0r2qh/wheels/39/3f/26/659151ec6871131bbec256d97eeebf33183b82936f0efea2c6\r\n",
      "Successfully built efficientnet\r\n",
      "Installing collected packages: efficientnet\r\n",
      "Successfully installed efficientnet-1.1.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install ../input/keras-efficientnet-whl/Keras_Applications-1.0.8-py3-none-any.whl\n",
    "# !pip install ../input/keras-efficientnet-whl/efficientnet-1.1.1-py3-none-any.whl\n",
    "!cp -r /kaggle/input/efficientnet-keras-dataset /kaggle/working/efficientnet-keras-dataset\n",
    "!pip install /kaggle/working/efficientnet-keras-dataset/efficientnet_kaggle\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/vit-keras-dataset')\n",
    "# sys.path.append('/kaggle/input/efficientnet100minimal')\n",
    "sys.path.append('/kaggle/input/nfnets-keras/nfnets_keras')\n",
    "!pip install -q /kaggle/input/validators0182-dataset/validators-0.18.2-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397dad35",
   "metadata": {
    "papermill": {
     "duration": 0.045481,
     "end_time": "2022-01-10T23:12:50.361900",
     "exception": false,
     "start_time": "2022-01-10T23:12:50.316419",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"imports\"> Importing Libraries üìö</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31dc4e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:50.460415Z",
     "iopub.status.busy": "2022-01-10T23:12:50.459203Z",
     "iopub.status.idle": "2022-01-10T23:12:57.945512Z",
     "shell.execute_reply": "2022-01-10T23:12:57.946407Z",
     "shell.execute_reply.started": "2022-01-10T13:13:01.555763Z"
    },
    "papermill": {
     "duration": 7.540148,
     "end_time": "2022-01-10T23:12:57.947003",
     "exception": false,
     "start_time": "2022-01-10T23:12:50.406855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import sklearn\n",
    "from glob import glob\n",
    "from tensorflow import keras\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf, re, math\n",
    "import efficientnet.tfkeras as efn\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd, numpy as np, random,os, shutil\n",
    "from vit_keras import vit, utils, visualize, layers\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ace33f",
   "metadata": {
    "papermill": {
     "duration": 0.086465,
     "end_time": "2022-01-10T23:12:58.126905",
     "exception": false,
     "start_time": "2022-01-10T23:12:58.040440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"config\"> Configuration ‚öôÔ∏è </span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf7d23cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:58.304053Z",
     "iopub.status.busy": "2022-01-10T23:12:58.301962Z",
     "iopub.status.idle": "2022-01-10T23:12:58.304781Z",
     "shell.execute_reply": "2022-01-10T23:12:58.305277Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.2364Z"
    },
    "papermill": {
     "duration": 0.092843,
     "end_time": "2022-01-10T23:12:58.305443",
     "exception": false,
     "start_time": "2022-01-10T23:12:58.212600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    competition   = 'petfinder'\n",
    "    debug         = False\n",
    "    exp_name      ='nuke_v1' # name of the experiment, folds will be grouped using 'exp_name'\n",
    "    \n",
    "    # USE verbose=0 for silent, vebose=1 for interactive, verbose=2 for commit\n",
    "    verbose      = 1 if debug else 0\n",
    "    display_plot = True\n",
    "\n",
    "    device = \"GPU\"\n",
    "\n",
    "    eat_model_name = 'eat'\n",
    "    dolg_model_name = 'dolg'\n",
    "    vit_model_name = 'vit_l16'\n",
    "    swin_model_name = 'swin-l'\n",
    "    nfnet_model_name = 'nfnet_f0'\n",
    "    efn_model_name = 'efficientnet_b7'\n",
    "    hybrid_swin_model_name = 'hybrid_swin'   \n",
    "    \n",
    "    # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD\n",
    "    seed = 42\n",
    "\n",
    "    # NUMBER OF FOLDS. USE 2, 5, 10\n",
    "    folds = 5\n",
    "    \n",
    "    # FOLDS TO TRAIN\n",
    "    selected_folds = [0, 1, 2, 3, 4]\n",
    "\n",
    "    # IMAGE SIZE\n",
    "    img_size = [512, 512]\n",
    "\n",
    "    # BATCH SIZE AND EPOCHS\n",
    "    batch_size  = 2 # 32\n",
    "    epochs      = 5\n",
    "\n",
    "    # LOSS\n",
    "    loss      = 'BCE'\n",
    "    optimizer = 'Adam'\n",
    "\n",
    "    # CFG.augmentATION\n",
    "    augment   = True\n",
    "    transform = True\n",
    "\n",
    "    # TRANSFORMATION\n",
    "    fill_mode = 'reflect'\n",
    "    rot    = 10.0\n",
    "    shr    = 5.0\n",
    "    hzoom  = 30.0\n",
    "    wzoom  = 30.0\n",
    "    hshift = 30.0\n",
    "    wshift = 30.0\n",
    "\n",
    "    # FLIP\n",
    "    hflip = True\n",
    "    vflip = True\n",
    "\n",
    "    # CLIP [0, 1]\n",
    "    clip = False\n",
    "\n",
    "    # LEARNING RATE SCHEDULER\n",
    "    scheduler   = 'exp' # Cosine\n",
    "\n",
    "    # Dropout\n",
    "    drop_prob   = 0.75\n",
    "    drop_cnt    = 10\n",
    "    drop_size   = 0.05\n",
    "\n",
    "    #bri, contrast\n",
    "    sat  = [0.7, 1.3]\n",
    "    cont = [0.8, 1.2]\n",
    "    bri  =  0.15\n",
    "    hue  = 0.05\n",
    "\n",
    "    # TEST TIME CFG.augmentATION STEPS\n",
    "    tta = 2\n",
    "    \n",
    "    tab_cols    = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "                   'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n",
    "    target_col  = ['Pawpularity']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab9005c",
   "metadata": {
    "papermill": {
     "duration": 0.04507,
     "end_time": "2022-01-10T23:12:58.397827",
     "exception": false,
     "start_time": "2022-01-10T23:12:58.352757",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"tpu_conf\"> TPU Configuration ‚öôÔ∏è </span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d69a5a0",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:58.502301Z",
     "iopub.status.busy": "2022-01-10T23:12:58.501558Z",
     "iopub.status.idle": "2022-01-10T23:12:58.707564Z",
     "shell.execute_reply": "2022-01-10T23:12:58.708346Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.251175Z"
    },
    "papermill": {
     "duration": 0.263467,
     "end_time": "2022-01-10T23:12:58.708594",
     "exception": false,
     "start_time": "2022-01-10T23:12:58.445127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using default strategy for CPU and single GPU\n",
      "Num GPUs Available:  1\n",
      "REPLICAS: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 23:12:58.567365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:12:58.695839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:12:58.697023: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "if CFG.device == \"TPU\":\n",
    "    print(\"connecting to TPU...\")\n",
    "    try:\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        print(\"Could not connect to TPU\")\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        try:\n",
    "            print(\"initializing  TPU ...\")\n",
    "            tf.config.experimental_connect_to_cluster(tpu)\n",
    "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "            print(\"TPU initialized\")\n",
    "        except _:\n",
    "            print(\"failed to initialize TPU\")\n",
    "    else:\n",
    "        CFG.device = \"GPU\"\n",
    "\n",
    "if CFG.device != \"TPU\":\n",
    "    print(\"Using default strategy for CPU and single GPU\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "if CFG.device == \"GPU\":\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "    \n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178b5c1",
   "metadata": {
    "papermill": {
     "duration": 0.046327,
     "end_time": "2022-01-10T23:12:58.804851",
     "exception": false,
     "start_time": "2022-01-10T23:12:58.758524",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"loading\"> Loading Meta Data üóÇÔ∏è </span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2225ed6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:58.905986Z",
     "iopub.status.busy": "2022-01-10T23:12:58.905320Z",
     "iopub.status.idle": "2022-01-10T23:12:58.960812Z",
     "shell.execute_reply": "2022-01-10T23:12:58.959619Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.434276Z"
    },
    "papermill": {
     "duration": 0.109254,
     "end_time": "2022-01-10T23:12:58.961001",
     "exception": false,
     "start_time": "2022-01-10T23:12:58.851747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/petfinder-pawpularity-score'\n",
    "df = pd.read_csv('../input/petfinder-pawpularity-score/train.csv')\n",
    "df['image_path'] = BASE_PATH + '/train/' + df.Id + '.jpg'\n",
    "test_df  = pd.read_csv('../input/petfinder-pawpularity-score/test.csv')\n",
    "test_df['image_path'] = BASE_PATH + '/test/' + test_df.Id + '.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4869fc6b",
   "metadata": {
    "papermill": {
     "duration": 0.047407,
     "end_time": "2022-01-10T23:12:59.056972",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.009565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"augmentations\"> Test Time Augmentations (TTA) ‚úÇÔ∏è </span>\n",
    "<hr>\n",
    "\n",
    "* Random - Horizontal Flip\n",
    "* Random - Brightness, Contrast, Hue, Saturation\n",
    "\n",
    "<img src=\"https://i.ibb.co/XC4dBJs/results-39-0.png\" alt=\"results-39-0\" border=\"0\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d00304",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:59.176729Z",
     "iopub.status.busy": "2022-01-10T23:12:59.175739Z",
     "iopub.status.idle": "2022-01-10T23:12:59.178916Z",
     "shell.execute_reply": "2022-01-10T23:12:59.178426Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.485011Z"
    },
    "papermill": {
     "duration": 0.076179,
     "end_time": "2022-01-10T23:12:59.179042",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.102863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mat(shear, height_zoom, width_zoom, height_shift, width_shift):\n",
    "    # returns 3x3 transformmatrix which transforms indicies\n",
    "        \n",
    "    # CONVERT DEGREES TO RADIANS\n",
    "    #rotation = math.pi * rotation / 180.\n",
    "    shear    = math.pi * shear    / 180.\n",
    "\n",
    "    def get_3x3_mat(lst):\n",
    "        return tf.reshape(tf.concat([lst],axis=0), [3,3])\n",
    "    \n",
    "    # ROTATION MATRIX\n",
    "#     c1   = tf.math.cos(rotation)\n",
    "#     s1   = tf.math.sin(rotation)\n",
    "    one  = tf.constant([1],dtype='float32')\n",
    "    zero = tf.constant([0],dtype='float32')\n",
    "    \n",
    "#     rotation_matrix = get_3x3_mat([c1,   s1,   zero, \n",
    "#                                    -s1,  c1,   zero, \n",
    "#                                    zero, zero, one])    \n",
    "    # SHEAR MATRIX\n",
    "    c2 = tf.math.cos(shear)\n",
    "    s2 = tf.math.sin(shear)    \n",
    "    \n",
    "    shear_matrix = get_3x3_mat([one,  s2,   zero, \n",
    "                               zero, c2,   zero, \n",
    "                                zero, zero, one])        \n",
    "    # ZOOM MATRIX\n",
    "    zoom_matrix = get_3x3_mat([one/height_zoom, zero,           zero, \n",
    "                               zero,            one/width_zoom, zero, \n",
    "                               zero,            zero,           one])    \n",
    "    # SHIFT MATRIX\n",
    "    shift_matrix = get_3x3_mat([one,  zero, height_shift, \n",
    "                                zero, one,  width_shift, \n",
    "                                zero, zero, one])\n",
    "    \n",
    "\n",
    "    return  K.dot(shear_matrix,K.dot(zoom_matrix, shift_matrix)) #K.dot(K.dot(rotation_matrix, shear_matrix), K.dot(zoom_matrix, shift_matrix))                  \n",
    "\n",
    "def transform(image, DIM=CFG.img_size):#[rot,shr,h_zoom,w_zoom,h_shift,w_shift]):\n",
    "    if DIM[0]!=DIM[1]:\n",
    "        pad = (DIM[0]-DIM[1])//2\n",
    "        image = tf.pad(image, [[0, 0], [pad, pad+1],[0, 0]])\n",
    "        \n",
    "    NEW_DIM = DIM[0]\n",
    "    \n",
    "    rot = CFG.rot * tf.random.normal([1], dtype='float32')\n",
    "    shr = CFG.shr * tf.random.normal([1], dtype='float32') \n",
    "    h_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG.hzoom\n",
    "    w_zoom = 1.0 + tf.random.normal([1], dtype='float32') / CFG.wzoom\n",
    "    h_shift = CFG.hshift * tf.random.normal([1], dtype='float32') \n",
    "    w_shift = CFG.wshift * tf.random.normal([1], dtype='float32') \n",
    "    \n",
    "    transformation_matrix=tf.linalg.inv(get_mat(shr,h_zoom,w_zoom,h_shift,w_shift))\n",
    "    \n",
    "    flat_tensor=tfa.image.transform_ops.matrices_to_flat_transforms(transformation_matrix)\n",
    "    \n",
    "    image=tfa.image.transform(image,flat_tensor, fill_mode=CFG.fill_mode)\n",
    "    \n",
    "    rotation = math.pi * rot / 180.\n",
    "    \n",
    "    image=tfa.image.rotate(image,-rotation, fill_mode=CFG.fill_mode)\n",
    "    \n",
    "    if DIM[0]!=DIM[1]:\n",
    "        image=tf.reshape(image, [NEW_DIM, NEW_DIM,3])\n",
    "        image = image[:, pad:DIM[1]+pad,:]\n",
    "    image = tf.reshape(image, [*DIM, 3])    \n",
    "    return image\n",
    "\n",
    "def dropout(image,DIM=CFG.img_size, PROBABILITY = 0.6, CT = 5, SZ = 0.1):\n",
    "    # input image - is one image of size [dim,dim,3] not a batch of [b,dim,dim,3]\n",
    "    # output - image with CT squares of side size SZ*DIM removed\n",
    "    \n",
    "    # DO DROPOUT WITH PROBABILITY DEFINED ABOVE\n",
    "    P = tf.cast( tf.random.uniform([],0,1)<PROBABILITY, tf.int32)\n",
    "    if (P==0)|(CT==0)|(SZ==0): \n",
    "        return image\n",
    "    \n",
    "    for k in range(CT):\n",
    "        # CHOOSE RANDOM LOCATION\n",
    "        x = tf.cast( tf.random.uniform([],0,DIM[1]),tf.int32)\n",
    "        y = tf.cast( tf.random.uniform([],0,DIM[0]),tf.int32)\n",
    "        # COMPUTE SQUARE \n",
    "        WIDTH = tf.cast( SZ*min(DIM),tf.int32) * P\n",
    "        ya = tf.math.maximum(0,y-WIDTH//2)\n",
    "        yb = tf.math.minimum(DIM[0],y+WIDTH//2)\n",
    "        xa = tf.math.maximum(0,x-WIDTH//2)\n",
    "        xb = tf.math.minimum(DIM[1],x+WIDTH//2)\n",
    "        # DROPOUT IMAGE\n",
    "        one = image[ya:yb,0:xa,:]\n",
    "        two = tf.zeros([yb-ya,xb-xa,3], dtype = image.dtype) \n",
    "        three = image[ya:yb,xb:DIM[1],:]\n",
    "        middle = tf.concat([one,two,three],axis=1)\n",
    "        image = tf.concat([image[0:ya,:,:],middle,image[yb:DIM[0],:,:]],axis=0)\n",
    "        image = tf.reshape(image,[*DIM,3])\n",
    "\n",
    "#     image = tf.reshape(image,[*DIM,3])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b34a80a",
   "metadata": {
    "papermill": {
     "duration": 0.047272,
     "end_time": "2022-01-10T23:12:59.272488",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.225216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"data_pipeline\"> Data Pipeline üîß </span>\n",
    "<hr>\n",
    "\n",
    "* Reads the raw file and then decodes it to tf.Tensor\n",
    "* Resizes the image in desired size\n",
    "* Chages the datatype to **float32**\n",
    "* Caches the Data for boosting up the speed.\n",
    "* Uses Augmentations to reduce overfitting and make model more robust.\n",
    "* Finally, splits the data into batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce478b84",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:59.385772Z",
     "iopub.status.busy": "2022-01-10T23:12:59.384705Z",
     "iopub.status.idle": "2022-01-10T23:12:59.387466Z",
     "shell.execute_reply": "2022-01-10T23:12:59.388045Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.512648Z"
    },
    "papermill": {
     "duration": 0.069749,
     "end_time": "2022-01-10T23:12:59.388225",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.318476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_decoder(with_labels=True, target_size=CFG.img_size, ext='jpg'):\n",
    "    def decode(path):\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        if ext == 'png':\n",
    "            img = tf.image.decode_png(file_bytes, channels=3)\n",
    "        elif ext in ['jpg', 'jpeg']:\n",
    "            img = tf.image.decode_jpeg(file_bytes, channels=3)\n",
    "        else:\n",
    "            raise ValueError(\"Image extension not supported\")\n",
    "\n",
    "        img = tf.image.resize(img, target_size)\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        img = tf.reshape(img, [*target_size, 3])\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return decode(path), tf.cast(label, tf.float32)\n",
    "    \n",
    "    return decode_with_labels if with_labels else decode\n",
    "\n",
    "\n",
    "def build_augmenter(with_labels=True, dim=CFG.img_size):\n",
    "    def augment(img, dim=dim):\n",
    "        img = transform(img,DIM=dim) if CFG.transform else img\n",
    "        img = tf.image.random_flip_left_right(img) if CFG.hflip else img\n",
    "        img = tf.image.random_flip_up_down(img) if CFG.vflip else img\n",
    "        img = tf.image.random_hue(img, CFG.hue)\n",
    "        img = tf.image.random_saturation(img, CFG.sat[0], CFG.sat[1])\n",
    "        img = tf.image.random_contrast(img, CFG.cont[0], CFG.cont[1])\n",
    "        img = tf.image.random_brightness(img, CFG.bri)\n",
    "        img = dropout(img, DIM=dim, PROBABILITY = CFG.drop_prob, CT = CFG.drop_cnt, SZ = CFG.drop_size)\n",
    "        img = tf.clip_by_value(img, 0, 1)  if CFG.clip else img         \n",
    "        img = tf.reshape(img, [*dim, 3])\n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):    \n",
    "        return augment(img), label\n",
    "    \n",
    "    return augment_with_labels if with_labels else augment\n",
    "\n",
    "\n",
    "def build_dataset(paths, labels=None, batch_size=32, cache=True,\n",
    "                  decode_fn=None, augment_fn=None,\n",
    "                  augment=True, repeat=True, shuffle=1024, \n",
    "                  cache_dir=\"\", drop_remainder=False):\n",
    "    if cache_dir != \"\" and cache is True:\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "    \n",
    "    if decode_fn is None:\n",
    "        decode_fn = build_decoder(labels is not None)\n",
    "    \n",
    "    if augment_fn is None:\n",
    "        augment_fn = build_augmenter(labels is not None)\n",
    "    \n",
    "    AUTO = tf.data.experimental.AUTOTUNE\n",
    "    slices = paths if labels is None else (paths, labels)\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.map(decode_fn, num_parallel_calls=AUTO)\n",
    "    ds = ds.cache(cache_dir) if cache else ds\n",
    "    ds = ds.repeat() if repeat else ds\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        ds = ds.with_options(opt)\n",
    "    ds = ds.map(augment_fn, num_parallel_calls=AUTO) if augment else ds\n",
    "    ds = ds.batch(batch_size, drop_remainder=drop_remainder)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db4172",
   "metadata": {
    "papermill": {
     "duration": 0.049081,
     "end_time": "2022-01-10T23:12:59.485350",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.436269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<h1> <span class=\"title-section w3-xxlarge\" id=\"the_models\"> Models Configuration ‚öôÔ∏è </span> </h1>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f500fe2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:59.586683Z",
     "iopub.status.busy": "2022-01-10T23:12:59.584918Z",
     "iopub.status.idle": "2022-01-10T23:12:59.589139Z",
     "shell.execute_reply": "2022-01-10T23:12:59.588550Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.53668Z"
    },
    "papermill": {
     "duration": 0.056384,
     "end_time": "2022-01-10T23:12:59.589301",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.532917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASE_DIRS = [\n",
    "    # (512, '../input/paw-model-no-aug'), # Models that trained without augmentations\n",
    "    # (512, '../input/paw-models'), # Models that trained with augmentations\n",
    "]\n",
    "\n",
    "MODEL_CONFIGS = []\n",
    "for dim, base_dir in  BASE_DIRS:\n",
    "    paths = sorted(glob(os.path.join(base_dir, '*h5')))\n",
    "    if len(paths) == 0: print('no model found for :',base_dir)\n",
    "    MODEL_CONFIGS.append([dim, paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb022a3",
   "metadata": {
    "papermill": {
     "duration": 0.046338,
     "end_time": "2022-01-10T23:12:59.681327",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.634989",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Competition Metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b143b443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:59.779737Z",
     "iopub.status.busy": "2022-01-10T23:12:59.778582Z",
     "iopub.status.idle": "2022-01-10T23:12:59.781554Z",
     "shell.execute_reply": "2022-01-10T23:12:59.782008Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.545869Z"
    },
    "papermill": {
     "duration": 0.054385,
     "end_time": "2022-01-10T23:12:59.782150",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.727765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def RMSE(y_true, y_pred, denormalize=True):\n",
    "    if denormalize:\n",
    "        y_true = y_true*100.0\n",
    "        y_pred = y_pred*100.0\n",
    "    loss = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(tf.subtract(y_true, y_pred))))\n",
    "    return loss\n",
    "RMSE.__name__='rmse'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a376242",
   "metadata": {
    "papermill": {
     "duration": 0.045248,
     "end_time": "2022-01-10T23:12:59.873044",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.827796",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<h1> <span class=\"title-section w3-xxlarge\" id=\"the_models\"> 1st Stage Models üöÄ </span> </h1> \n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6e64be",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:12:59.971293Z",
     "iopub.status.busy": "2022-01-10T23:12:59.970342Z",
     "iopub.status.idle": "2022-01-10T23:12:59.973835Z",
     "shell.execute_reply": "2022-01-10T23:12:59.973194Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.554632Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.054316,
     "end_time": "2022-01-10T23:12:59.973984",
     "exception": false,
     "start_time": "2022-01-10T23:12:59.919668",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_types = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fe7ca7",
   "metadata": {
    "papermill": {
     "duration": 0.046335,
     "end_time": "2022-01-10T23:13:00.066846",
     "exception": false,
     "start_time": "2022-01-10T23:13:00.020511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center><h1><span class=\"title-section w3-xxlarge\" id=\"swin\"> ‚ò¢Ô∏è Swin Transformer ‚ò¢Ô∏è</span></h1></center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**You cannot say \"swin transformer\" without saying \"WIN\"**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Swin transformers seem to be a game-changer in many computer vision tasks including object detection, image classification, semantic segmentation, and potentially any vision task. Applications of Transformers in vision problems the initial ViT(Vision Transformer) showed promising performance but adapting Transformers to fully supplement convolutions was still considered a challenge. \n",
    "\n",
    "Swin transformers on the other hand can model the differences between the two domains such as variations in the scale of objects and the high resolution of pixels in images more efficiently and can serve as a general-purpose pipeline for vision.\n",
    "\n",
    "The original paper describes Swin Transformers as a hierarchical Transformer whose representation is computed with Shifted WINdows. \n",
    "\n",
    "Swin Transformer Main Contribution: \n",
    "\n",
    "- Hierarchical representation by starting from small-sized patches and gradually increasing the size through merging to achieve scale-invariance\n",
    "\n",
    "- Achieves efficient, linear computational complexity by computing self-attention locally. (shifted window approach)\n",
    "\n",
    "- Extensive experiments on various tasks and model architectures.\n",
    "\n",
    "\n",
    "![](https://user-images.githubusercontent.com/17668390/145635292-0e439077-7695-4cd6-8a41-8ba259f08ec4.png)\n",
    "\n",
    "\n",
    "For further reading, see [this](https://sieunpark77.medium.com/swin-transformers-the-most-powerful-tool-in-computer-vision-659f78744871) great summary\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cdd449c",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:13:00.192258Z",
     "iopub.status.busy": "2022-01-10T23:13:00.171652Z",
     "iopub.status.idle": "2022-01-10T23:17:57.101364Z",
     "shell.execute_reply": "2022-01-10T23:17:57.100667Z",
     "shell.execute_reply.started": "2022-01-10T13:13:08.562311Z"
    },
    "papermill": {
     "duration": 296.989054,
     "end_time": "2022-01-10T23:17:57.101519",
     "exception": false,
     "start_time": "2022-01-10T23:13:00.112465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4444    100\n",
      "3076     65\n",
      "2455     12\n",
      "8270     57\n",
      "7232     29\n",
      "Name: Pawpularity, dtype: int64 tensor([[0.67476],\n",
      "        [0.40575],\n",
      "        [0.23867],\n",
      "        [0.31538],\n",
      "        [0.37513]])\n",
      "Fold 0 | Score: 16.282230645455243\n",
      "CV Score: 16.282230645455243\n"
     ]
    }
   ],
   "source": [
    "!cp -R '/kaggle/input/yolov5/torch/root/.cache/torch' '/root/.cache/torch'\n",
    "!cp -R '/kaggle/input/yolov5/ultralytics/root/.config/Ultralytics' '/root/.config/Ultralytics'\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append('/kaggle/input')\n",
    "\n",
    "import os\n",
    "if not os.path.exists('/root/.cache/torch/hub/checkpoints/'):\n",
    "    os.makedirs('/root/.cache/torch/hub/checkpoints/')\n",
    "\n",
    "os.system(\"cp -R '/kaggle/input/yolov5/torch/root/.cache/torch' '/root/.cache/torch'\")\n",
    "os.system(\"cp -R '/kaggle/input/yolov5/ultralytics/root/.config/Ultralytics' '/root/.config/Ultralytics'\")\n",
    "\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/timm-pytorch-image-models/pytorch-image-models-master')\n",
    "sys.path.append('/kaggle/input')\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import imageio\n",
    "from timm import create_model\n",
    "from fastai.vision.all import *\n",
    "from timm.data.mixup import Mixup\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "set_seed(1, reproducible=True)\n",
    "BATCH_SIZE = 32\n",
    "NEED_TRAIN = True\n",
    "FORCE_TRAINING = False\n",
    "\n",
    "clean_dataset_path = Path('/kaggle/input/pawpularity-dataset')\n",
    "clean_dataset_path.ls()\n",
    "\n",
    "dataset_path = Path('/kaggle/input/petfinder-pawpularity-score/')\n",
    "dataset_path.ls()\n",
    "\n",
    "train_df = pd.read_csv(clean_dataset_path/'pawpularity_train_manipulated.csv')\n",
    "train_df.head()\n",
    "\n",
    "train_df['path'] = train_df['Id'].map(lambda x:str(dataset_path/'train'/x)+'.jpg')\n",
    "dropped = ['Id', 'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']\n",
    "train_df = train_df.drop(columns=dropped)\n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "features = ['Pawpularity', 'cat', 'dog', 'neither']\n",
    "train_df[features] = train_df[features].astype('int')\n",
    "train_df['Pawpularity'].hist(figsize = (10, 5))\n",
    "train_df['norm_score'] = train_df['Pawpularity'] / 100\n",
    "\n",
    "im = Image.open(train_df['path'][1])\n",
    "if not os.path.exists('/root/.cache/torch/hub/checkpoints/'): os.makedirs('/root/.cache/torch/hub/checkpoints/')\n",
    "os.system(\"cp '/kaggle/input/swin-transformer/swin_large_patch4_window7_224_22kto1k.pth' '/root/.cache/torch/hub/checkpoints/swin_large_patch4_window7_224_22kto1k.pth'\")\n",
    "\n",
    "seed = 12\n",
    "set_seed(seed, reproducible=True)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.use_deterministic_algorithms = True\n",
    "num_bins = int(np.floor(1+np.log2(len(train_df))))\n",
    "train_df['bins'] = pd.cut(train_df['norm_score'], bins=num_bins, labels=False)\n",
    "train_df['fold'] = -1\n",
    "\n",
    "N_FOLDS = 10\n",
    "strat_kfold = StratifiedKFold(n_splits=N_FOLDS, random_state=seed, shuffle=True)\n",
    "for i, (_, train_index) in enumerate(strat_kfold.split(train_df.index, train_df['bins'])): train_df.iloc[train_index, -1] = i\n",
    "    \n",
    "train_df['fold'] = train_df['fold'].astype('int')\n",
    "train_df.fold.value_counts().plot.bar()\n",
    "train_df[train_df['fold']==0].head()\n",
    "train_df[train_df['fold']==0]['bins'].value_counts()\n",
    "train_df[train_df['fold']==1]['bins'].value_counts()\n",
    "\n",
    "def petfinder_rmse(input,target): return 100*torch.sqrt(F.mse_loss(F.sigmoid(input.flatten()), target))\n",
    "\n",
    "def get_data(fold):\n",
    "    train_df_f = train_df.copy()\n",
    "    train_df_f['is_valid'] = (train_df_f['fold'] == fold)\n",
    "    splitter = RandomSplitter(0.2)\n",
    "    splitter = IndexSplitter(splitter(range(len(train_df)))[1])\n",
    "    dls = DataBlock(blocks=(ImageBlock, RegressionBlock),\n",
    "                get_x=ColReader('path'),\n",
    "                get_y=ColReader('norm_score'),\n",
    "                splitter=splitter,\n",
    "                item_tfms=Resize(224),\n",
    "                batch_tfms=setup_aug_tfms([Flip()])\n",
    "               )\n",
    "    paw_dls = dls.dataloaders(train_df_f, \n",
    "                          bs=BATCH_SIZE,\n",
    "                          num_workers=8,\n",
    "                          seed=seed)\n",
    "    return paw_dls, splitter\n",
    "\n",
    "def get_learner(fold_num):\n",
    "    data, splitter = get_data(fold_num)\n",
    "    model = create_model('swin_large_patch4_window7_224', pretrained=True, num_classes=data.c)\n",
    "    learn = Learner(data, model, loss_func=BCEWithLogitsLossFlat(), metrics=petfinder_rmse, cbs=[MixUp(0.2)]).to_fp16()\n",
    "    return learn, splitter\n",
    "\n",
    "def add_labels(df):\n",
    "    df[['cat', 'dog', 'neither']] = 0\n",
    "    ohe = []\n",
    "    for idx, path in zip(itertools.count(), df['path'].values):\n",
    "        img = imageio.imread(path)\n",
    "        result = yolov5(img)\n",
    "        labels = result.pandas().xyxy[0]['name'].values\n",
    "        found_label = False\n",
    "        for label in labels:\n",
    "            if label == 'cat':\n",
    "                ohe.append([1,0,0])\n",
    "                found_label = True\n",
    "                break\n",
    "            elif label == 'dog':\n",
    "                ohe.append([0,1,0])\n",
    "                found_label = True\n",
    "                break\n",
    "        if not found_label:\n",
    "            ohe.append([0,0,1])\n",
    "    df[['cat', 'dog', 'neither']] = ohe\n",
    "    return df\n",
    "\n",
    "yolov5 = torch.hub.load('ultralytics/yolov5', 'yolov5x6')\n",
    "test_df = pd.read_csv(dataset_path/'test.csv')\n",
    "if len(test_df) != 8: NEED_TRAIN = True\n",
    "test_df['Pawpularity'] = [1]*len(test_df)\n",
    "test_df['path'] = test_df['Id'].map(lambda x:str(dataset_path/'test'/x)+'.jpg')\n",
    "test_df = test_df.drop(columns=dropped)\n",
    "add_labels(test_df)\n",
    "train_df['norm_score'] = train_df['Pawpularity']/100\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "if NEED_TRAIN:\n",
    "    all_preds = []\n",
    "    train_df['pred'] = -1\n",
    "    for i in range(N_FOLDS):\n",
    "        print(f'Fold {i} results')\n",
    "        learn, splitter = get_learner(fold_num=i)\n",
    "        if FORCE_TRAINING:\n",
    "            learn.fit_one_cycle(5, 2e-5, cbs=[SaveModelCallback(), EarlyStoppingCallback(monitor='petfinder_rmse', comp=np.less, patience=2)])\n",
    "            learn.recorder.plot_loss()\n",
    "            learn.export(f'yolo_model_fold_{i}.pkl')\n",
    "        else: learn = load_learner(f'/kaggle/input/yolov5-swin-transformer-trained/yolo_model_fold_{i}.pkl', cpu = False)\n",
    "        dls = DataBlock(blocks=(ImageBlock, RegressionBlock),\n",
    "                    get_x=ColReader('path'),\n",
    "                    get_y=ColReader('norm_score'),\n",
    "                    splitter=RandomSplitter(0.2),\n",
    "                    item_tfms=Resize(224),\n",
    "                    batch_tfms=setup_aug_tfms([Flip()]))\n",
    "        paw_dls = dls.dataloaders(train_df, bs=BATCH_SIZE, num_workers=8, seed=seed)\n",
    "        test_dl = paw_dls.test_dl(test_df)\n",
    "        preds, _ = learn.tta(dl=test_dl, n=5, beta=0)\n",
    "        all_preds.append(preds * 100.0)\n",
    "        val_idx = splitter(range(len(train_df)))[1]\n",
    "        val_df = train_df.loc[val_idx]\n",
    "        val_pred, _ = learn.tta(dl=paw_dls.test_dl(val_df), n=5, beta=0)\n",
    "        print(val_df['Pawpularity'][:5], val_pred[:5])\n",
    "        score = mean_squared_error(val_df['Pawpularity'], val_pred*100, squared=False)\n",
    "        print(f'Fold {i} | Score: {score}')\n",
    "        train_df.loc[val_idx, 'pred'] = val_pred*100\n",
    "        del learn\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        if len(test_df) == 8 and (not FORCE_TRAINING): break\n",
    "    if (len(test_df) == 8) or FORCE_TRAINING:\n",
    "        cv_score = mean_squared_error(train_df.loc[train_df['pred']!=-1, 'Pawpularity'], train_df.loc[train_df['pred']!=-1, 'pred'], squared=False)\n",
    "        print(f'CV Score: {cv_score}')\n",
    "\n",
    "if NEED_TRAIN: all_preds, np.mean(np.stack(all_preds))\n",
    "sample_df = pd.read_csv(dataset_path/'sample_submission.csv')\n",
    "if NEED_TRAIN:\n",
    "    preds = np.mean(np.stack(all_preds), axis=0)\n",
    "    sample_df['Pawpularity'] = preds*100\n",
    "sample_df.to_csv('submission.csv',index=False)\n",
    "if not NEED_TRAIN: pd.read_csv('submission.csv').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfee67d",
   "metadata": {
    "papermill": {
     "duration": 0.050818,
     "end_time": "2022-01-10T23:17:57.205501",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.154683",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center> <h1> <span class=\"title-section w3-xxlarge\" id=\"vit\"> ‚ò¢Ô∏è Vision Transformer (ViT) ‚ò¢Ô∏è </span> </h1> </center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "[**How it works?**](https://analyticsindiamag.com/hands-on-vision-transformers-with-pytorch/)\n",
    "\n",
    "Paper: [‚ÄúAn Image is Worth 16√ó16 Words: Transformers for Image Recognition at Scale‚Äù](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "* ViT breaks an input image of 16√ó16 to a  sequence of patches, just like a series of word embeddings generated by an NLP Transformers. \n",
    "* Each patch gets flattened into a single vector in a series of interconnected channels of all pixels in a patch, then projects it to desired input dimension.\n",
    "* Because transformers operate in self-attention mode, and they do not necessarily depend on the structure of the input elements, which in turns helps the architecture to learn and relate sparsely-distributed information more efficiently.\n",
    "* The relationship between the patches in an image is not known and thus allows it to learn more relevant features from the training data and encode in positional embedding in ViT. \n",
    "\n",
    "<img src=\"https://lh5.googleusercontent.com/xk2mQd9H3w4uS482ursxhbDZhr7UXxJ9RMZ7VVjErBMuhbsB1QfSary9pOWU4P5EeZHmB05R8KalB5GXx__eCiN2AQ5qRhXY4vHwYe2zoFqIO0XkpHHXIE8VP99lpcgW5HtjPKKx\" width=\"800\">\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af864192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:57.321183Z",
     "iopub.status.busy": "2022-01-10T23:17:57.319000Z",
     "iopub.status.idle": "2022-01-10T23:17:57.321989Z",
     "shell.execute_reply": "2022-01-10T23:17:57.322546Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.011178Z"
    },
    "papermill": {
     "duration": 0.065157,
     "end_time": "2022-01-10T23:17:57.322737",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.257580",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vit_keras import vit, utils, visualize, layers\n",
    "\n",
    "def build_model_vit(model_name=CFG.vit_model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):\n",
    "    base = getattr(vit, model_name)(image_size=(DIM),\n",
    "                   include_top=False, \n",
    "                   pretrained_top=False,\n",
    "                   pretrained=False) \n",
    "                   # weights='imagenet21k+imagenet2012')    \n",
    "    # base.load_weights('../input/vit-16-l-weights/vit_l_16.h5')\n",
    "    inp = base.inputs\n",
    "    out = base.output    \n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "    if compile_model:\n",
    "        #optimizer\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        #loss\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n",
    "        #metric\n",
    "        rmse = RMSE\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=loss,\n",
    "                      metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "#model_types['vit_l16'] = build_model_vit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9164f68a",
   "metadata": {
    "papermill": {
     "duration": 0.050294,
     "end_time": "2022-01-10T23:17:57.425168",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.374874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center> <h1> <span class=\"title-section w3-xxlarge\" id=\"nfnet\"> ‚ò¢Ô∏è Normalization Free Network (NFNet) ‚ò¢Ô∏è </span> </h1> </center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "**About NFNet**: I am the guy behind \"nfnets-keras\" (you can pip install it). If you are interested: Have a look at my [github](https://github.com/ypeleg/nfnets-keras). I used it on the shopee competition earlier this year and ended up with a silver. Enjoy! \n",
    "\n",
    "Quick summary about NFNets: \n",
    "**NFNets ‚Äî State-of-the-art normalizer free networks**\n",
    "\n",
    "Without going into too much detail, NFNets are a family of modified ResNets that achieves competitive accuracies **without** batch normalization. To do so, it uses 3 different tricks:\n",
    "- Modified residual branches and convolutions with Scaled Weight Standardization\n",
    "- Adaptive Gradient Clipping\n",
    "- Architecture optimization for improved accuracy and training speed\n",
    "\n",
    "These 3 techniques are complex topics on its own, so I won't go into all three at the moment, I will just cover the architecture improvements since this is what is used here: \n",
    "\n",
    "**Modified residual branches and convolutions**\n",
    "In order to train deep ResNets without normalization, it is crucial to suppress the scale of the activations on the residual branch. To achieve this, NFNets uses 2 scalers (Œ± and Œ≤) to scale the activations at the start and end of the residual branch.\n",
    "![](https://i.ibb.co/1mW38Zw/1-l-BYi-AADnyr-CWxoa-Fkqug-Wg.png)\n",
    "Œ± is set to a small constant of 0.2, while Œ≤ for each block is defined as:\n",
    "$$Œ≤_i=\\sqrt(Var(h_i))$$\n",
    "\n",
    "where, \n",
    "\n",
    "$$Var(h_i)=Var(h_(i-1)) + Œ±^{2}$$\n",
    "\n",
    "and h refers to the inputs to the block.\n",
    "In addition, NFNets uses Scaled Weight Standardization to prevent mean-shift. Scaled Weight Standardization normalizes the weights of the convolutional layers in NFNets such that:\n",
    "\n",
    "$$W_ij = \\frac{W_ij - Œº_i}{\\sqrt(N)œÉ_i}$$\n",
    "\n",
    "For anyone interested in reading more, please checkout this blog (where the image came from):\n",
    "https://towardsdatascience.com/nfnets-explained-deepminds-new-state-of-the-art-image-classifier-10430c8599ee\n",
    "\n",
    "____\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db6c9e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:57.539222Z",
     "iopub.status.busy": "2022-01-10T23:17:57.538046Z",
     "iopub.status.idle": "2022-01-10T23:17:57.607185Z",
     "shell.execute_reply": "2022-01-10T23:17:57.606552Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.02447Z"
    },
    "papermill": {
     "duration": 0.130305,
     "end_time": "2022-01-10T23:17:57.607330",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.477025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nfnets_keras.nfnet import NFNetF0, NFNetF1, NFNetF2, NFNetF3, NFNetF4, NFNetF5, NFNetF6, NFNetF7\n",
    "\n",
    "name2nfnet = {\n",
    "    'nfnet_f0': NFNetF0,\n",
    "    'nfnet_f1': NFNetF1,\n",
    "    'nfnet_f2': NFNetF2,\n",
    "    'nfnet_f3': NFNetF3,\n",
    "    'nfnet_f4': NFNetF4,\n",
    "    'nfnet_f5': NFNetF5,\n",
    "    'nfnet_f6': NFNetF6,\n",
    "    'nfnet_f7': NFNetF7,\n",
    "}\n",
    "\n",
    "def build_model_nfnet(model_name=CFG.nfnet_model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):\n",
    "\n",
    "    inp = tf.keras.layers.Input((DIM, DIM, 3))\n",
    "    base = name2nfnet[model_name](num_classes = 1000, include_top = include_top)\n",
    "    # base.load_weights(f\"/home/user/Desktop/F0_NFNet/F0_NFNet\")\n",
    "    \n",
    "    out = base(inp)\n",
    "    out = tf.keras.layers.GlobalAveragePooling2D()(out)    \n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "    if compile_model:\n",
    "        #optimizer\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        #loss\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n",
    "        #metric\n",
    "        rmse = RMSE\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=loss,\n",
    "                      metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "model_types['nfnet'] = build_model_nfnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ef180",
   "metadata": {
    "papermill": {
     "duration": 0.050144,
     "end_time": "2022-01-10T23:17:57.710449",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.660305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center> <h1> <span class=\"title-section w3-xxlarge\" id=\"efficientnet\"> ‚ò¢Ô∏è Efficientnet B7 ‚ò¢Ô∏è </span> </h1> </center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "EfficientNet model was proposed by Mingxing Tan and Quoc V. Le of Google Research, Brain team in their research paper ‚ÄòEfficientNet: Rethinking Model Scaling for Convolutional Neural Networks‚Äô. This paper was presented in the International Conference on Machine Learning, 2019. These researchers studied the model scaling and identified that carefully balancing the depth, width, and resolution of the network can lead to better performance.\n",
    "\n",
    "Based on this observation, they proposed a new scaling method that uniformly scales all dimensions of depth, width and resolution of the network. They used the neural architecture search to design a new baseline network and scaled it up to obtain a family of deep learning models, called EfficientNets, which achieve much better accuracy and efficiency as compared to the previous Convolutional Neural Networks.\n",
    "\n",
    "Scaling\n",
    "The researchers used the compound scaling method to scale the dimensions of the network. The applied grid search strategy to find the relationship between the different scaling dimensions of the baseline network under a fixed resource constraint. Using this strategy, the could find the appropriate scaling coefficients for each of the dimensions to be scaled-up. Using these coefficients, the baseline network was scaled by the desired size.\n",
    "\n",
    "![](https://i.ibb.co/mXsj866/scaling.png)\n",
    "\n",
    "Image: Original Research [Paper](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "\n",
    "The researchers claimed in their work that this compound scaling method improved the model‚Äôs accuracy and efficiency. \n",
    "\n",
    "(Summary from [here](https://analyticsindiamag.com/implementing-efficientnet-a-powerful-convolutional-neural-network/))\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c698d32f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:57.822078Z",
     "iopub.status.busy": "2022-01-10T23:17:57.819984Z",
     "iopub.status.idle": "2022-01-10T23:17:57.822890Z",
     "shell.execute_reply": "2022-01-10T23:17:57.823483Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.091102Z"
    },
    "papermill": {
     "duration": 0.063354,
     "end_time": "2022-01-10T23:17:57.823662",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.760308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "name2effnet = {\n",
    "    'efficientnet_b0': efn.EfficientNetB0,\n",
    "    'efficientnet_b1': efn.EfficientNetB1,\n",
    "    'efficientnet_b2': efn.EfficientNetB2,\n",
    "    'efficientnet_b3': efn.EfficientNetB3,\n",
    "    'efficientnet_b4': efn.EfficientNetB4,\n",
    "    'efficientnet_b5': efn.EfficientNetB5,\n",
    "    'efficientnet_b6': efn.EfficientNetB6,\n",
    "    'efficientnet_b7': efn.EfficientNetB7,\n",
    "}\n",
    "\n",
    "def build_model_efn(model_name=CFG.efn_model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):\n",
    "    base = name2effnet[model_name](input_shape=(DIM, DIM, 3),\n",
    "                                  include_top=include_top,\n",
    "                                  weights=None,\n",
    "                                  )\n",
    "    inp = base.inputs\n",
    "    out = base.output\n",
    "    out = tf.keras.layers.GlobalAveragePooling2D()(out)    \n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(out)\n",
    "    model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "    if compile_model:\n",
    "        #optimizer\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        #loss\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n",
    "        #metric\n",
    "        rmse = RMSE\n",
    "        model.compile(optimizer=opt,\n",
    "                      loss=loss,\n",
    "                      metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "model_types['efficientnet'] = build_model_efn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1461b1c",
   "metadata": {
    "papermill": {
     "duration": 0.048567,
     "end_time": "2022-01-10T23:17:57.922501",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.873934",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center> <h1> <span class=\"title-section w3-xxlarge\" id=\"hybrid_swin\"> ‚ò¢Ô∏è Hybrid Swin Transformer ‚ò¢Ô∏è </span> </h1> </center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "**Hybrid EfficientNet Swin Transformer** model, This approach was used in Google Landmark Recognition 2021 competition by [Dieter](https://www.kaggle.com/christofhenkel) originally in PyTorch, solution explained [here](https://www.kaggle.com/c/landmark-recognition-2021/discussion/277098) in great details.\n",
    "\n",
    "> **Credit:** This Implementation if from the great [notebook](https://www.kaggle.com/ipythonx/tf-keras-efficientnet-hybrid-swin-transformer-tpu) by M.Innat.\n",
    "\n",
    "![](https://i.imgur.com/2iXNuBA.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb9e4838",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:58.032654Z",
     "iopub.status.busy": "2022-01-10T23:17:58.031453Z",
     "iopub.status.idle": "2022-01-10T23:17:58.105513Z",
     "shell.execute_reply": "2022-01-10T23:17:58.104913Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.103089Z"
    },
    "papermill": {
     "duration": 0.132419,
     "end_time": "2022-01-10T23:17:58.105654",
     "exception": false,
     "start_time": "2022-01-10T23:17:57.973235",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "patch_size = (2, 2)  # 4-by-4 sized patches\n",
    "dropout_rate = 0.5  # Dropout rate\n",
    "num_heads = 8  # Attention heads\n",
    "embed_dim = 64  # Embedding dimension\n",
    "num_mlp = 128  # MLP layer size\n",
    "qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n",
    "window_size = 2  # Size of attention window\n",
    "shift_size = 1  # Size of shifting window\n",
    "image_dimension = 512  # Initial image size / Input size of the transformer model\n",
    "\n",
    "num_patch_x = image_dimension // (patch_size[0] ** 2)\n",
    "num_patch_y = image_dimension // (patch_size[1] ** 2)\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    _, height, width, channels = x.shape\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(x, shape = (-1, patch_num_y, window_size, patch_num_x, window_size, channels))\n",
    "    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n",
    "    windows = tf.reshape(x, shape = (-1, window_size, window_size, channels))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, height, width, channels):\n",
    "    patch_num_y = height // window_size\n",
    "    patch_num_x = width // window_size\n",
    "    x = tf.reshape(windows, shape = (-1, patch_num_y, patch_num_x, window_size, window_size, channels), )\n",
    "    x = tf.transpose(x, perm = (0, 1, 3, 2, 4, 5))\n",
    "    x = tf.reshape(x, shape = (-1, height, width, channels))\n",
    "    return x\n",
    "\n",
    "\n",
    "class DropPath(layers.Layer):\n",
    "    def __init__(self, drop_prob = None, **kwargs):\n",
    "        super(DropPath, self).__init__(**kwargs)\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def call(self, x):\n",
    "        input_shape = tf.shape(x)\n",
    "        batch_size = input_shape[0]\n",
    "        rank = x.shape.rank\n",
    "        shape = (batch_size,) + (1,) * (rank - 1)\n",
    "        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype = x.dtype)\n",
    "        path_mask = tf.floor(random_tensor)\n",
    "        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n",
    "        return output\n",
    "\n",
    "\n",
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias = True, dropout_rate = 0.0, **kwargs):\n",
    "        super(WindowAttention, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias = qkv_bias)\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        num_window_elements = (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1)\n",
    "        self.relative_position_bias_table = self.add_weight(shape = (num_window_elements, self.num_heads), initializer = tf.initializers.Zeros(), trainable = True)\n",
    "        coords_h = np.arange(self.window_size[0])\n",
    "        coords_w = np.arange(self.window_size[1])\n",
    "        coords_matrix = np.meshgrid(coords_h, coords_w, indexing = \"ij\")\n",
    "        coords = np.stack(coords_matrix)\n",
    "        coords_flatten = coords.reshape(2, -1)\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "        relative_coords = relative_coords.transpose([1, 2, 0])\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.relative_position_index = tf.Variable(initial_value = tf.convert_to_tensor(relative_position_index), trainable = False)\n",
    "\n",
    "    def call(self, x, mask = None):\n",
    "        _, size, channels = x.shape\n",
    "        head_dim = channels // self.num_heads\n",
    "        x_qkv = self.qkv(x)\n",
    "        x_qkv = tf.reshape(x_qkv, shape = (-1, size, 3, self.num_heads, head_dim))\n",
    "        x_qkv = tf.transpose(x_qkv, perm = (2, 0, 3, 1, 4))\n",
    "        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n",
    "        q = q * self.scale\n",
    "        k = tf.transpose(k, perm = (0, 1, 3, 2))\n",
    "        attn = q @ k\n",
    "\n",
    "        num_window_elements = self.window_size[0] * self.window_size[1]\n",
    "        relative_position_index_flat = tf.reshape(self.relative_position_index, shape = (-1,))\n",
    "        relative_position_bias = tf.gather(self.relative_position_bias_table, relative_position_index_flat)\n",
    "        relative_position_bias = tf.reshape(relative_position_bias, shape = (num_window_elements, num_window_elements, -1))\n",
    "        relative_position_bias = tf.transpose(relative_position_bias, perm = (2, 0, 1))\n",
    "        attn = attn + tf.expand_dims(relative_position_bias, axis = 0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.get_shape()[0]\n",
    "            mask_float = tf.cast(tf.expand_dims(tf.expand_dims(mask, axis = 1), axis = 0), tf.float32)\n",
    "            attn = (tf.reshape(attn, shape = (-1, nW, self.num_heads, size, size)) + mask_float)\n",
    "            attn = tf.reshape(attn, shape = (-1, self.num_heads, size, size))\n",
    "            attn = keras.activations.softmax(attn, axis = -1)\n",
    "        else:\n",
    "            attn = keras.activations.softmax(attn, axis = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        x_qkv = attn @ v\n",
    "        x_qkv = tf.transpose(x_qkv, perm = (0, 2, 1, 3))\n",
    "        x_qkv = tf.reshape(x_qkv, shape = (-1, size, channels))\n",
    "        x_qkv = self.proj(x_qkv)\n",
    "        x_qkv = self.dropout(x_qkv)\n",
    "        return x_qkv\n",
    "\n",
    "class SwinTransformer(layers.Layer):\n",
    "    def __init__(self, dim, num_patch, num_heads, window_size = 7, shift_size = 0, num_mlp = 1024, qkv_bias = True, dropout_rate = 0.0, **kwargs, ):\n",
    "        super(SwinTransformer, self).__init__(**kwargs)\n",
    "        self.dim = dim  # number of input dimensions\n",
    "        self.num_patch = num_patch  # number of embedded patches\n",
    "        self.num_heads = num_heads  # number of attention heads\n",
    "        self.window_size = window_size  # size of window\n",
    "        self.shift_size = shift_size  # size of window shift\n",
    "        self.num_mlp = num_mlp  # number of MLP nodes\n",
    "        self.norm1 = layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.attn = WindowAttention(dim, window_size = (self.window_size, self.window_size), num_heads = num_heads, qkv_bias = qkv_bias, dropout_rate = dropout_rate, )\n",
    "        self.drop_path = DropPath(dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon = 1e-5)\n",
    "        self.mlp = keras.Sequential([layers.Dense(num_mlp), layers.Activation(keras.activations.gelu), layers.Dropout(dropout_rate), layers.Dense(dim), layers.Dropout(dropout_rate), ])\n",
    "        if min(self.num_patch) < self.window_size:\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.num_patch)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.shift_size == 0:\n",
    "            self.attn_mask = None\n",
    "        else:\n",
    "            height, width = self.num_patch\n",
    "            h_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None),)\n",
    "            w_slices = (slice(0, -self.window_size), slice(-self.window_size, -self.shift_size), slice(-self.shift_size, None),)\n",
    "            mask_array = np.zeros((1, height, width, 1))\n",
    "            count = 0\n",
    "            for h in h_slices:\n",
    "                for w in w_slices:\n",
    "                    mask_array[:, h, w, :] = count\n",
    "                    count += 1\n",
    "            mask_array = tf.convert_to_tensor(mask_array)\n",
    "            mask_windows = window_partition(mask_array, self.window_size)\n",
    "            mask_windows = tf.reshape(mask_windows, shape = [-1, self.window_size * self.window_size])\n",
    "            attn_mask = tf.expand_dims(mask_windows, axis = 1) - tf.expand_dims(mask_windows, axis = 2)\n",
    "            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n",
    "            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n",
    "            self.attn_mask = tf.Variable(initial_value = attn_mask, trainable = False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, num_patches_before, channels = x.shape\n",
    "        x_skip = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, shape = (-1, height, width, channels))\n",
    "        if self.shift_size > 0: shifted_x = tf.roll(x, shift = [-self.shift_size, -self.shift_size], axis = [1, 2])\n",
    "        else: shifted_x = x\n",
    "        x_windows = window_partition(shifted_x, self.window_size)\n",
    "        x_windows = tf.reshape(x_windows, shape = (-1, self.window_size * self.window_size, channels))\n",
    "        attn_windows = self.attn(x_windows, mask = self.attn_mask)\n",
    "        attn_windows = tf.reshape(attn_windows, shape = (-1, self.window_size, self.window_size, channels))\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, height, width, channels)\n",
    "        if self.shift_size > 0: x = tf.roll(shifted_x, shift = [self.shift_size, self.shift_size], axis = [1, 2])\n",
    "        else: x = shifted_x\n",
    "        x = tf.reshape(x, shape = (-1, height * width, channels))\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        x_skip = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = self.drop_path(x)\n",
    "        x = x_skip + x\n",
    "        return x\n",
    "\n",
    "class PatchExtract(layers.Layer):\n",
    "    def __init__(self, patch_size, **kwargs):\n",
    "        super(PatchExtract, self).__init__(**kwargs)\n",
    "        self.patch_size_x = patch_size[0]\n",
    "        self.patch_size_y = patch_size[0]\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(images = images, sizes = (1, self.patch_size_x, self.patch_size_y, 1), strides = (1, self.patch_size_x, self.patch_size_y, 1), rates = (1, 1, 1, 1), padding = \"VALID\", )\n",
    "        patch_dim = patches.shape[-1]\n",
    "        patch_num = patches.shape[1]\n",
    "        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n",
    "\n",
    "\n",
    "class PatchEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim, **kwargs):\n",
    "        super(PatchEmbedding, self).__init__(**kwargs)\n",
    "        self.num_patch = num_patch\n",
    "        self.proj = layers.Dense(embed_dim)\n",
    "        self.pos_embed = layers.Embedding(input_dim = num_patch, output_dim = embed_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        pos = tf.range(start = 0, limit = self.num_patch, delta = 1)\n",
    "        return self.proj(patch) + self.pos_embed(pos)\n",
    "\n",
    "\n",
    "class PatchMerging(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_patch, embed_dim):\n",
    "        super(PatchMerging, self).__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.embed_dim = embed_dim\n",
    "        self.linear_trans = layers.Dense(2 * embed_dim, use_bias = False)\n",
    "\n",
    "    def call(self, x):\n",
    "        height, width = self.num_patch\n",
    "        _, _, C = x.get_shape().as_list()\n",
    "        x = tf.reshape(x, shape = (-1, height, width, C))\n",
    "        feat_maps = x\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, :]\n",
    "        x3 = x[:, 1::2, 1::2, :]\n",
    "        x = tf.concat((x0, x1, x2, x3), axis = -1)\n",
    "        x = tf.reshape(x, shape = (-1, (height // 2) * (width // 2), 4 * C))\n",
    "        return self.linear_trans(x), feat_maps\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model, Sequential, Input, layers, applications\n",
    "\n",
    "BASE_WEIGHTS = '../input/efficientnet-keras-noisystudent-weights-b0b7/'\n",
    "IMG_NET = ['imagenet/imagenet.notop-b0.h5', 'imagenet/imagenet.notop-b1.h5', 'imagenet/imagenet.notop-b2.h5']\n",
    "NOISY_STUD = ['noisystudent/noisy.student.notop-b0.h5', 'noisystudent/noisy.student.notop-b1.h5', 'noisystudent/noisy.student.notop-b2.h5', ]\n",
    "\n",
    "class HybridSwinTransformer(Model):\n",
    "    def __init__(self, img_size, class_number):\n",
    "        self.img_size = img_size\n",
    "        self.class_number = class_number\n",
    "        super(HybridSwinTransformer, self).__init__()\n",
    "        # base models\n",
    "        self.inputx = Input((self.img_size, self.img_size, 3), name = 'input_hybrids')\n",
    "        base = applications.EfficientNetB0(include_top = False, weights = 'imagenet', input_tensor = self.inputx)\n",
    "        # base model with compatible output which will be an input of transformer model\n",
    "        self.new_base = Model([base.inputs], [base.get_layer('block6a_expand_activation').output, base.output],  # output with 192 feat_maps\n",
    "            name = 'efficientnet')\n",
    "\n",
    "        # stuff of swin transformers\n",
    "        self.patch_extract = PatchExtract(patch_size)\n",
    "        print(num_patch_x * num_patch_y, embed_dim)\n",
    "        self.patch_embedds = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)\n",
    "        self.patch_merging = PatchMerging((num_patch_x, num_patch_y), embed_dim = embed_dim)\n",
    "\n",
    "        self.swin_sequences = keras.Sequential(name = 'swin_blocks')\n",
    "        for i in range(shift_size):\n",
    "            self.swin_sequences.add(SwinTransformer(dim = embed_dim, num_patch = (num_patch_x, num_patch_y), num_heads = num_heads, window_size = window_size, shift_size = i, num_mlp = num_mlp, qkv_bias = qkv_bias, dropout_rate = dropout_rate))\n",
    "\n",
    "        self.swin_head = Sequential([layers.GlobalAveragePooling1D(), layers.AlphaDropout(0.5), layers.BatchNormalization(), ], name = 'swin_head')\n",
    "\n",
    "        self.conv_head = Sequential([layers.GlobalAveragePooling2D(), layers.AlphaDropout(0.5), ], name = 'conv_head')\n",
    "\n",
    "        self.classifier = layers.Dense(self.class_number, activation = 'sigmoid')\n",
    "\n",
    "    # ma-ma calling\n",
    "    def call(self, inputs, training = None, **kwargs):\n",
    "        x, base_top = self.new_base(inputs)\n",
    "\n",
    "        x = self.patch_extract(x)\n",
    "        x = self.patch_embedds(x)\n",
    "        x = self.swin_sequences(x)\n",
    "        x, swin_gcam_top = self.patch_merging(x)\n",
    "\n",
    "        swin_top = self.swin_head(x)\n",
    "        conv_top = self.conv_head(base_top)\n",
    "        preds = self.classifier(tf.concat([swin_top, conv_top], axis = -1))\n",
    "\n",
    "        if training:  # training phase\n",
    "            return preds\n",
    "        else:  # inference phase\n",
    "            return preds, base_top, swin_gcam_top\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape = (self.img_size, self.img_size, 3))\n",
    "        return Model(inputs = [x], outputs = self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f3dd77d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:58.214534Z",
     "iopub.status.busy": "2022-01-10T23:17:58.213516Z",
     "iopub.status.idle": "2022-01-10T23:17:58.216672Z",
     "shell.execute_reply": "2022-01-10T23:17:58.216060Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.180246Z"
    },
    "papermill": {
     "duration": 0.059522,
     "end_time": "2022-01-10T23:17:58.216798",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.157276",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_hybrid_swin(model_name=CFG.hybrid_swin_model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):\n",
    "    model = HybridSwinTransformer(DIM, 1)\n",
    "    model = model.build_graph()\n",
    "    if compile_model:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n",
    "        rmse = RMSE\n",
    "        model.compile(optimizer=opt, loss=loss, metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "# model_types['hybrid_swin'] = build_model_hybrid_swin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7909d7cc",
   "metadata": {
    "papermill": {
     "duration": 0.049214,
     "end_time": "2022-01-10T23:17:58.316137",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.266923",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center><h1><span class=\"title-section w3-xxlarge\" id=\"dolg\"> ‚ò¢Ô∏è DOLG ‚ò¢Ô∏è </span></h1></center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Deep Orthogonal Fusion of Local and Global Features (DOLG)**\n",
    "\n",
    "The following variation (DOLG + EfficientNet) was used as part of the winning solution (1st) of Google Landmark Recognition and Retrieval Comps.\n",
    "\n",
    "The main idea is to train local and global descriptors simultaneously but additionally fuse them within the model into a single descriptor. \n",
    "\n",
    "> **Credit:** This Implementation if from the great [repository](https://github.com/innat/DOLG-TensorFlow) by M.Innat.\n",
    "\n",
    "![](https://i.imgur.com/9FiSEet.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21b06840",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:58.434128Z",
     "iopub.status.busy": "2022-01-10T23:17:58.428141Z",
     "iopub.status.idle": "2022-01-10T23:17:58.460790Z",
     "shell.execute_reply": "2022-01-10T23:17:58.460016Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.194644Z"
    },
    "papermill": {
     "duration": 0.095505,
     "end_time": "2022-01-10T23:17:58.460976",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.365471",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import applications, layers, Model, Input\n",
    "from tensorflow.keras import (layers, Sequential, activations, initializers)\n",
    "\n",
    "class MultiAtrous(tf.keras.Model):\n",
    "    def __init__(self, dilation_rates = [6, 12, 18], upsampling = 1, kernel_size = 3, padding = \"same\", **kwargs):\n",
    "        super(MultiAtrous, self).__init__(name = 'MultiAtrous', **kwargs)\n",
    "        self.dilation_rates = dilation_rates\n",
    "        self.kernel_size = kernel_size\n",
    "        self.upsampling = upsampling\n",
    "        self.padding = padding\n",
    "        self.dilated_convs = [layers.Conv2D(filters = int(1024 / 4), kernel_size = self.kernel_size, padding = self.padding, dilation_rate = rate) for rate in self.dilation_rates]\n",
    "        self.gap_branch = Sequential([layers.Lambda(lambda t4d: K.mean(t4d, axis = (1, 2), keepdims = True), name = 'GlobalAverage2D'), layers.Conv2D(int(1024 / 2), kernel_size = 1), layers.Activation('relu'), layers.UpSampling2D(size = self.upsampling, interpolation = \"bilinear\")], name = 'gap_branch')\n",
    "\n",
    "    def call(self, inputs, training = None, **kwargs):\n",
    "        local_feature = []\n",
    "        for dilated_conv in self.dilated_convs:\n",
    "            x = dilated_conv(inputs)\n",
    "            x = self.gap_branch(x)\n",
    "            local_feature.append(x)\n",
    "        return tf.concat(local_feature, axis = -1)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'dilation_rates': self.dilation_rates, 'kernel_size': self.kernel_size, 'padding': self.padding, 'upsampling': self.upsampling}\n",
    "        base_config = super(MultiAtrous, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class DOLGLocalBranch(tf.keras.Model):\n",
    "    def __init__(self, img_size, **kwargs):\n",
    "        super(DOLGLocalBranch, self).__init__(name = 'LocalBranch', **kwargs)\n",
    "        self.multi_atrous = MultiAtrous(padding = 'same', upsampling = int(img_size / 32))\n",
    "        self.conv1 = layers.Conv2D(1024, kernel_size = 1)\n",
    "        self.conv2 = layers.Conv2D(1024, kernel_size = 1, use_bias = False)\n",
    "        self.conv3 = layers.Conv2D(1024, kernel_size = 1)\n",
    "        self.bn = layers.BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training = None, **kwargs):\n",
    "        local_feat = self.multi_atrous(inputs)\n",
    "        local_feat = self.conv1(local_feat)\n",
    "        local_feat = tf.nn.relu(local_feat)\n",
    "        local_feat = self.conv2(local_feat)\n",
    "        local_feat = self.bn(local_feat)\n",
    "        norm_local_feat = tf.math.l2_normalize(local_feat)\n",
    "        attn_map = tf.nn.relu(local_feat)\n",
    "        attn_map = self.conv3(attn_map)\n",
    "        attn_map = activations.softplus(attn_map)\n",
    "        return norm_local_feat * attn_map\n",
    "\n",
    "class OrthogonalFusion(layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(name = 'OrthogonalFusion', **kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        local_feat, global_feat = inputs\n",
    "        height = local_feat.shape[1]\n",
    "        width = local_feat.shape[2]\n",
    "        depth = local_feat.shape[3]\n",
    "\n",
    "        local_feat = tf.reshape(local_feat, [-1, height * width, depth])\n",
    "        local_feat = tf.transpose(local_feat, perm = [0, 2, 1])\n",
    "\n",
    "        projection = tf.matmul(tf.expand_dims(global_feat, axis = 1), local_feat)\n",
    "        projection = tf.matmul(tf.expand_dims(global_feat, axis = 2), projection)\n",
    "        projection = tf.reshape(projection, [-1, height, width, depth])\n",
    "\n",
    "        global_feat_norm = tf.norm(global_feat, ord = 2, axis = 1)\n",
    "        projection = projection / tf.reshape(global_feat_norm * global_feat_norm, shape = [-1, 1, 1, 1])\n",
    "        local_feat = tf.transpose(local_feat, perm = [0, 1, 2])\n",
    "        local_feat = tf.reshape(local_feat, [-1, height, width, depth])\n",
    "\n",
    "        orthogonal_comp = local_feat - projection\n",
    "        global_feat = tf.expand_dims(tf.expand_dims(global_feat, axis = 1), axis = 1)\n",
    "        global_feat = tf.broadcast_to(global_feat, tf.shape(local_feat))\n",
    "        output = tf.concat([global_feat, orthogonal_comp], axis = -1)\n",
    "        return output\n",
    "\n",
    "class GeneralizedMeanPooling2D(layers.Layer):\n",
    "    def __init__(self, init_norm = 3.0, normalize = False, epsilon = 1e-6, **kwargs):\n",
    "        self.init_norm = init_norm\n",
    "        self.normalize = normalize\n",
    "        self.epsilon = epsilon\n",
    "        super(GeneralizedMeanPooling2D, self).__init__(name = 'GeM', **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.p = self.add_weight(name = \"norms\", shape = (input_shape[-1],), initializer = initializers.constant(self.init_norm), trainable = True)\n",
    "        super(GeneralizedMeanPooling2D, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reduce_mean(tf.abs(inputs ** self.p), axis = [1, 2], keepdims = False) + self.epsilon\n",
    "        x = x ** (1.0 / self.p)\n",
    "        if self.normalize:\n",
    "            x = tf.nn.l2_normalize(x, 1)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'init_norm': self.init_norm, 'normalize': self.normalize, 'epsilon': self.epsilon}\n",
    "        base_config = super(GeneralizedMeanPooling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class DOLGNet(tf.keras.Model):\n",
    "    def __init__(self, img_size, Classifier, **kwargs):\n",
    "        self.img_size = img_size\n",
    "        super(DOLGNet, self).__init__(name = 'DOLGNet', **kwargs)\n",
    "        self.Classifier = Classifier\n",
    "        self.orthogonal_fusion = OrthogonalFusion()\n",
    "        self.local_branch = DOLGLocalBranch(img_size)\n",
    "        self.glob_branch_pool = Sequential([layers.GlobalAveragePooling2D(), layers.Dense(1024, activation = None)], name = 'GlobalBranchPooling')\n",
    "        base = applications.EfficientNetB5(include_top = False, weights = 'imagenet', input_tensor = Input((img_size, img_size, 3)))\n",
    "        self.new_base = Model([base.inputs], [base.get_layer('block5g_add').output,\n",
    "            base.get_layer('block7c_add').output\n",
    "        ], name = 'EfficientNet')\n",
    "        if Classifier == 1: self.classifier = Sequential([layers.GlobalAveragePooling2D(name = 'HeadGAP'), layers.Dense(1, activation = 'sigmoid')], name = 'Classifiers')\n",
    "        else: self.classifier = Sequential([layers.GlobalAveragePooling2D(name = 'HeadGAP'), layers.Dense(Classifier, activation = 'softmax')], name = 'Classifiers')\n",
    "    def call(self, inputs, training = None, **kwargs):\n",
    "        to_local, to_global = self.new_base(inputs)\n",
    "        local_feat = self.local_branch(to_local)\n",
    "        global_feat = self.glob_branch_pool(to_global)\n",
    "        orthogonal_feat = self.orthogonal_fusion([local_feat, global_feat])\n",
    "        if training: return self.classifier(orthogonal_feat)\n",
    "        else: return self.classifier(orthogonal_feat), orthogonal_feat\n",
    "\n",
    "    def build_graph(self):\n",
    "        x = Input(shape = (self.img_size, self.img_size, 3))\n",
    "        return Model(inputs = [x], outputs = self.call(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "611799af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:58.566999Z",
     "iopub.status.busy": "2022-01-10T23:17:58.562641Z",
     "iopub.status.idle": "2022-01-10T23:17:58.570505Z",
     "shell.execute_reply": "2022-01-10T23:17:58.569850Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.237128Z"
    },
    "papermill": {
     "duration": 0.059808,
     "end_time": "2022-01-10T23:17:58.570631",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.510823",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model_dolg(model_name=CFG.hybrid_swin_model_name, DIM=CFG.img_size[0], compile_model=True, include_top=False):\n",
    "\n",
    "    model = DOLGNet(img_size = CFG.img_size[0], Classifier = 1)\n",
    "    model = model.build_graph()\n",
    "    model.summary()\n",
    "\n",
    "    if compile_model:\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n",
    "        model.compile(optimizer=opt, loss=loss)\n",
    "    return model\n",
    "\n",
    "# model_types['dolg'] = build_model_dolg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1675934a",
   "metadata": {
    "papermill": {
     "duration": 0.049411,
     "end_time": "2022-01-10T23:17:58.669329",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.619918",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center><h1><span class=\"title-section w3-xxlarge\" id=\"swin\"> ‚ò¢Ô∏è Swin Transformer ‚ò¢Ô∏è</span></h1></center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "**You cannot say \"swin transformer\" without saying \"WIN\"**\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Swin transformers seem to be a game-changer in many computer vision tasks including object detection, image classification, semantic segmentation, and potentially any vision task. Applications of Transformers in vision problems the initial ViT(Vision Transformer) showed promising performance but adapting Transformers to fully supplement convolutions was still considered a challenge. \n",
    "\n",
    "Swin transformers on the other hand can model the differences between the two domains such as variations in the scale of objects and the high resolution of pixels in images more efficiently and can serve as a general-purpose pipeline for vision.\n",
    "\n",
    "The original paper describes Swin Transformers as a hierarchical Transformer whose representation is computed with Shifted WINdows. \n",
    "\n",
    "Swin Transformer Main Contribution: \n",
    "\n",
    "- Hierarchical representation by starting from small-sized patches and gradually increasing the size through merging to achieve scale-invariance\n",
    "\n",
    "- Achieves efficient, linear computational complexity by computing self-attention locally. (shifted window approach)\n",
    "\n",
    "- Extensive experiments on various tasks and model architectures.\n",
    "\n",
    "\n",
    "![](https://user-images.githubusercontent.com/17668390/145635292-0e439077-7695-4cd6-8a41-8ba259f08ec4.png)\n",
    "\n",
    "\n",
    "For further reading, see [this](https://sieunpark77.medium.com/swin-transformers-the-most-powerful-tool-in-computer-vision-659f78744871) great summary\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c19fc6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:58.779870Z",
     "iopub.status.busy": "2022-01-10T23:17:58.778683Z",
     "iopub.status.idle": "2022-01-10T23:17:58.804612Z",
     "shell.execute_reply": "2022-01-10T23:17:58.804113Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.24813Z"
    },
    "papermill": {
     "duration": 0.085976,
     "end_time": "2022-01-10T23:17:58.804742",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.718766",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "sys.path.append('/kaggle/input/swintransformertf')\n",
    "from swintransformer import SwinTransformer\n",
    "\n",
    "class Mish(tf.keras.layers.Activation):\n",
    "    def __init__(self, activation, **kwargs):\n",
    "        super(Mish, self).__init__(activation, **kwargs)\n",
    "        self.__name__ = 'Mish'\n",
    "\n",
    "def mish(inputs): return inputs * tf.math.tanh(tf.math.softplus(inputs))\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'Mish': Mish(mish)})\n",
    "\n",
    "def get_centralized_gradients(optimizer, loss, params):\n",
    "    grads = []\n",
    "    for grad in K.gradients(loss, params):\n",
    "        grad_len = len(grad.shape)\n",
    "        if grad_len > 1:\n",
    "            axis = list(range(grad_len - 1))\n",
    "            grad -= tf.reduce_mean(grad, axis=axis, keep_dims=True)\n",
    "        grads.append(grad)\n",
    "    if None in grads: raise ValueError('An operation has `None` for gradient. Please make sure that all of your ops have a gradient defined (i.e. are differentiable). Common ops without gradient: K.argmax, K.round, K.eval.')\n",
    "    if hasattr(optimizer, 'clipnorm') and optimizer.clipnorm > 0:\n",
    "        norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n",
    "        grads = [tf.keras.optimizers.clip_norm(g, optimizer.clipnorm, norm) for g in grads]\n",
    "    if hasattr(optimizer, 'clipvalue') and optimizer.clipvalue > 0: grads = [K.clip(g, -optimizer.clipvalue, optimizer.clipvalue) for g in grads]\n",
    "    return grads\n",
    "\n",
    "def centralized_gradients_for_optimizer(optimizer):\n",
    "    def get_centralized_gradients_for_optimizer(loss, params): return get_centralized_gradients(optimizer, loss, params)\n",
    "    return get_centralized_gradients_for_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c32bc4e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:58.921037Z",
     "iopub.status.busy": "2022-01-10T23:17:58.920155Z",
     "iopub.status.idle": "2022-01-10T23:17:58.923587Z",
     "shell.execute_reply": "2022-01-10T23:17:58.924113Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.283311Z"
    },
    "papermill": {
     "duration": 0.06882,
     "end_time": "2022-01-10T23:17:58.924260",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.855440",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_swin(model_name=CFG.swin_model_name, DIM=384, compile_model=True, include_top=False):\n",
    "    image_input = tf.keras.layers.Input(shape=(DIM, DIM, 3))\n",
    "    pretrained_model = SwinTransformer(model_name, include_top=False, pretrained=True, use_tpu=False)\n",
    "    base = tf.keras.Sequential([pretrained_model,\n",
    "                                tf.keras.layers.Dropout(0.25),\n",
    "                                tf.keras.layers.Dense(1, activation='sigmoid')])\n",
    "\n",
    "    x = tf.keras.layers.Lambda(lambda image: tf.image.resize(image, (384, 384)))(image_input)\n",
    "    x = base(x)\n",
    "    model = tf.keras.Model(inputs = image_input, outputs = x)\n",
    "    if compile_model:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        optimizer.get_gradients=centralized_gradients_for_optimizer(optimizer)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n",
    "        rmse = RMSE\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "model_types['swin_transformer'] = build_model_swin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aeb641",
   "metadata": {
    "papermill": {
     "duration": 0.050017,
     "end_time": "2022-01-10T23:17:59.023999",
     "exception": false,
     "start_time": "2022-01-10T23:17:58.973982",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "<center><h1><span class=\"title-section w3-xxlarge\" id=\"eat\"> ‚ò¢Ô∏è External Attention Transformer (EAT) ‚ò¢Ô∏è </span></h1></center>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "The author of this paper paper propos a new lightweight attention mechanism: External Attention. \n",
    "\n",
    "In classic self attention, the calculation starts by computing self queries (self-query) Vectors and self bonds (self-key). The affinity between vectors to compute an attention graph, Then a new feature graph is generated by weighting the self-worth vector with the attention graph. \n",
    "\n",
    "The proposed external attention works differently: First compute the self query vector and the external learnable key memory (key-memory) To calculate the affinity between the attention graph, And then by multiplying this attention graph by another external learnable value (value-memory) To produce a perfect feature map.\n",
    "\n",
    "Both of these memory units are done through the linear layer, so it can be optimized by back gradient propagation. They are independent of a single sample, And share it throughout the dataset, This has a powerful regularization effect that improves the generalization ability of the proposed attention mechanism.\n",
    "\n",
    "> **Credit:** This Implementation if from the great [repository](https://github.com/innat/External-Attention-TensorFlow) by M.Innat.\n",
    "\n",
    "\n",
    "![](https://user-images.githubusercontent.com/17668390/141291708-7c3cd892-d508-4cca-8306-a8b06a38c158.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be027adc",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:59.162292Z",
     "iopub.status.busy": "2022-01-10T23:17:59.161192Z",
     "iopub.status.idle": "2022-01-10T23:17:59.164057Z",
     "shell.execute_reply": "2022-01-10T23:17:59.164664Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.29488Z"
    },
    "papermill": {
     "duration": 0.090978,
     "end_time": "2022-01-10T23:17:59.164839",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.073861",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbed(tf.keras.layers.Layer):\n",
    "    def __init__(self, img_size=(224, 224), patch_size=(4, 4),  embed_dim=96):\n",
    "        super().__init__(name='patch_embed')\n",
    "        patches_resolution = [img_size[0] // patch_size[0],\n",
    "                              img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "        self.embed_dim = embed_dim\n",
    "        self.proj = layers.Conv2D(embed_dim,\n",
    "                                  kernel_size=patch_size,\n",
    "                                  strides=patch_size, name='proj')\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-5, name='norm')\n",
    "\n",
    "    def call(self, x):\n",
    "        B, H, W, C = x.get_shape().as_list()\n",
    "        x = self.proj(x)\n",
    "        x = tf.reshape(\n",
    "            x, shape=[-1,\n",
    "                      (H // self.patch_size[0]) * (W // self.patch_size[0]),\n",
    "                      self.embed_dim]\n",
    "        )\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ExternalAttention(layers.Layer):\n",
    "    def __init__(self, dim, num_heads, dim_coefficient = 4,\n",
    "                 attention_dropout = 0,  projection_dropout = 0,\n",
    "                 **kwargs):\n",
    "        super(ExternalAttention, self).__init__(name= 'ExternalAttention', **kwargs)\n",
    "        self.dim       = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_coefficient    = dim_coefficient\n",
    "        self.attention_dropout  = attention_dropout\n",
    "        self.projection_dropout = projection_dropout\n",
    "\n",
    "        k = 256 // dim_coefficient\n",
    "        self.trans_dims = layers.Dense(dim * dim_coefficient)\n",
    "        self.linear_0 = layers.Dense(k)\n",
    "        self.linear_1 = layers.Dense(dim * dim_coefficient // num_heads)\n",
    "        self.proj = layers.Dense(dim)\n",
    "\n",
    "        self.attn_drop  = layers.Dropout(attention_dropout)\n",
    "        self.proj_drop  = layers.Dropout(projection_dropout)\n",
    "\n",
    "    def call(self, inputs, return_attention_scores=False, training=None):\n",
    "        num_patch = tf.shape(inputs)[1]\n",
    "        channel   = tf.shape(inputs)[2]\n",
    "        x = self.trans_dims(inputs)\n",
    "        x = tf.reshape(x, shape=(-1,\n",
    "                                 num_patch,\n",
    "                                 self.num_heads,\n",
    "                                 self.dim * self.dim_coefficient // self.num_heads))\n",
    "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # a linear layer M_k\n",
    "        attn = self.linear_0(x)\n",
    "        # normalize attention map\n",
    "        attn = layers.Softmax(axis=2)(attn)\n",
    "        # dobule-normalization\n",
    "        attn = attn / (1e-9 + tf.reduce_sum(attn, axis=-1, keepdims=True))\n",
    "        attn_drop = self.attn_drop(attn, training=training)\n",
    "\n",
    "        # a linear layer M_v\n",
    "        attn_dense = self.linear_1(attn_drop)\n",
    "        x = tf.transpose(attn_dense, perm=[0, 2, 1, 3])\n",
    "        x = tf.reshape(x, [-1, num_patch, self.dim * self.dim_coefficient])\n",
    "        # a linear layer to project original dim\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x, training=training)\n",
    "\n",
    "        if return_attention_scores:\n",
    "            return x, attn\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'dim'                : self.dim,\n",
    "            'num_heads'          : self.num_heads,\n",
    "            'dim_coefficient'    : self.dim_coefficient,\n",
    "            'attention_dropout'  : self.attention_dropout,\n",
    "            'projection_dropout' : self.projection_dropout\n",
    "        }\n",
    "        base_config = super(ExternalAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class MLP(layers.Layer):\n",
    "    def __init__(self, mlp_dim, embedding_dim=None,\n",
    "                 act_layer=tf.nn.gelu, drop_rate=0.2, **kwargs):\n",
    "        super(MLP, self).__init__(name='MLP', **kwargs)\n",
    "        self.fc1  = layers.Dense(mlp_dim, activation=act_layer)\n",
    "        self.fc2  = layers.Dense(embedding_dim)\n",
    "        self.drop = layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.drop(x, training=training)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x, training=training)\n",
    "        return x\n",
    "\n",
    "patch_size       = 4\n",
    "num_heads        = 4\n",
    "embedding_dim    = 128\n",
    "mlp_dim          = 64\n",
    "dim_coefficient  = 4\n",
    "num_patches      = (CFG.img_size[0] // patch_size) ** 2\n",
    "attention_dropout   = 0.2\n",
    "projection_dropout  = 0.2\n",
    "num_ext_transformer_blocks = 10\n",
    "\n",
    "\n",
    "class AttentionEncoder(layers.Layer):\n",
    "    def __init__(self, embedding_dim,\n",
    "                 mlp_dim, num_heads,\n",
    "                 dim_coefficient,\n",
    "                 attention_dropout,\n",
    "                 projection_dropout,\n",
    "                 get_attention_matrix=False,\n",
    "                 **kwargs):\n",
    "        super(AttentionEncoder, self).__init__(**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mlp_dim   = mlp_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dim_coefficient    = dim_coefficient\n",
    "        self.attention_dropout  = attention_dropout\n",
    "        self.projection_dropout = projection_dropout\n",
    "        self.get_attention_matrix = get_attention_matrix\n",
    "        self.mlp = MLP(mlp_dim, embedding_dim)\n",
    "\n",
    "        self.etn = ExternalAttention(\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            dim_coefficient,\n",
    "            attention_dropout,\n",
    "            projection_dropout\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        residual_1 = inputs\n",
    "        x, ext_attention_scores = self.etn(inputs, return_attention_scores=True)\n",
    "        x = layers.add([x, residual_1])\n",
    "        residual_2 = x\n",
    "        x = self.mlp(x)\n",
    "        x = layers.add([x, residual_2])\n",
    "\n",
    "        if self.get_attention_matrix:\n",
    "            return x, ext_attention_scores\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'embedding_dim'     : self.embedding_dim,\n",
    "            'mlp_dim'           : self.mlp_dim,\n",
    "            'num_heads'         : self.num_heads,\n",
    "            'dim_coefficient'   : self.dim_coefficient,\n",
    "            'attention_dropout' : self.attention_dropout,\n",
    "            'projection_dropout': self.projection_dropout,\n",
    "            'get_attention_matrix': self.get_attention_matrix\n",
    "        }\n",
    "        base_config = super(AttentionEncoder, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb402bc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:59.267498Z",
     "iopub.status.busy": "2022-01-10T23:17:59.266491Z",
     "iopub.status.idle": "2022-01-10T23:17:59.279325Z",
     "shell.execute_reply": "2022-01-10T23:17:59.278745Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.333247Z"
    },
    "papermill": {
     "duration": 0.064622,
     "end_time": "2022-01-10T23:17:59.279455",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.214833",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_eat(model_name=CFG.eat_model_name, DIM=384, compile_model=True, include_top=False):\n",
    "    image_inputs  = layers.Input((DIM, DIM, 3))\n",
    "    backbone = tf.keras.applications.ResNet50(include_top = False, weights = None, input_tensor = layers.Input((DIM, DIM, 3)), )\n",
    "    multi_op_backbone = Model(backbone.input, [backbone.get_layer('conv2_block3_out').output, backbone.output])\n",
    "    mid_y, last_y = multi_op_backbone(image_inputs)\n",
    "    patchedx = PatchEmbed(img_size = (DIM, DIM), patch_size = (patch_size, patch_size), embed_dim = embedding_dim)(mid_y)\n",
    "    x = patchedx\n",
    "    for _ in range(num_ext_transformer_blocks): x, attn_weight_matrix = AttentionEncoder(embedding_dim, mlp_dim, num_heads, dim_coefficient, attention_dropout, projection_dropout, get_attention_matrix = True)(x)\n",
    "    tail_1 = Sequential([layers.GlobalAveragePooling1D(), layers.Dropout(0.5), layers.BatchNormalization()], name = 'tail_1')\n",
    "    tail_2 = Sequential([layers.GlobalAveragePooling2D(), layers.Dropout(0.5), ], name = 'tail_2')\n",
    "    cating = tf.concat([tail_1(x), tail_2(last_y)], axis = -1)\n",
    "    classifier = layers.Dense(1, activation = 'sigmoid')(cating)\n",
    "    model = Model([image_inputs], classifier)\n",
    "\n",
    "    if compile_model:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        optimizer.get_gradients=centralized_gradients_for_optimizer(optimizer)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n",
    "        rmse = RMSE\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=[rmse])\n",
    "    return model\n",
    "\n",
    "# model_types['eat'] = build_model_eat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc776e1",
   "metadata": {
    "papermill": {
     "duration": 0.04868,
     "end_time": "2022-01-10T23:17:59.376875",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.328195",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"first_infer\"> 1st Stage Inference üî•</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b87cd5da",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:59.493633Z",
     "iopub.status.busy": "2022-01-10T23:17:59.479694Z",
     "iopub.status.idle": "2022-01-10T23:17:59.542211Z",
     "shell.execute_reply": "2022-01-10T23:17:59.532067Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.350863Z"
    },
    "papermill": {
     "duration": 0.117379,
     "end_time": "2022-01-10T23:17:59.542423",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.425044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================\n",
      "### Inference\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe485b7cb2744b018c44c7d786e7dd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('='*35)\n",
    "print('### Inference')\n",
    "print('='*35)\n",
    "preds = []\n",
    "first_stage_preds = []\n",
    "for dim, model_paths in tqdm(MODEL_CONFIGS):\n",
    "    test_paths = test_df.image_path.tolist()\n",
    "    if len(test_paths)<=8:\n",
    "        CFG.batch_size = 1\n",
    "    elif dim>=768: CFG.batch_size = REPLICAS * 16\n",
    "    elif dim>=640: CFG.batch_size = REPLICAS * 24\n",
    "    else: CFG.batch_size = REPLICAS * 32\n",
    "    dtest = build_dataset(\n",
    "        test_paths, \n",
    "        batch_size=CFG.batch_size, repeat=True, \n",
    "        shuffle=False, augment=True if CFG.tta>1 else False, cache=False,\n",
    "        decode_fn=build_decoder(with_labels=False, target_size=[dim,dim]),\n",
    "        augment_fn=build_augmenter(with_labels=False, dim=[dim, dim])\n",
    "    )\n",
    "    for model_path in model_paths:\n",
    "        print(f'Model: {model_path}')\n",
    "        with strategy.scope():\n",
    "            print('Loading Model...')\n",
    "            model = None\n",
    "            if 'nfnet' in model_path: model = build_model_nfnet('nfnet_f0', 512)                \n",
    "            elif 'vit' in model_path: model = build_model_vit('vit_l16', 512)\n",
    "            elif 'eff' in model_path: model = build_model_efn('efficientnet_b7', 512)                \n",
    "            elif 'eat' in model_path: model = build_model_eat('eat', 512)                \n",
    "            elif 'dolg' in model_path: model = build_model_dolg('dolg', 512)                \n",
    "            elif 'swin-l' in model_path: model = build_model_swin('swin-l', 512)\n",
    "            elif 'hybrid_swin' in model_path: model = build_hybrid_swin('hybrid_swin', 512)            \n",
    "            if model is not None: model.load_weights(model_path)            \n",
    "            # model = tf.keras.models.load_model(model_path, compile=False)\n",
    "        if model is not None: \n",
    "            print('Predicting...');\n",
    "            pred = model.predict(dtest, steps = CFG.tta*len(test_paths)/CFG.batch_size, verbose=1)\n",
    "            pred = pred[:CFG.tta*len(test_paths),:]\n",
    "\n",
    "            pred = np.median(pred.reshape(CFG.tta, len(test_paths), -1), axis=0)\n",
    "\n",
    "            preds.append(pred*100.0) # denormalizing from [0-1] to [0-100]\n",
    "            first_stage_preds.append(pred*100.0)\n",
    "            print()\n",
    "\n",
    "# preds = np.median(np.concatenate([np.expand_dims(pred, axis = 0) for pred in preds], axis = 0), axis=0)\n",
    "# nets_preds = np.mean(np.concatenate([np.expand_dims(pred, axis = 0) for pred in preds], axis = 0), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8003a1bd",
   "metadata": {
    "papermill": {
     "duration": 0.050194,
     "end_time": "2022-01-10T23:17:59.641941",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.591747",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"first_submit\"> Submission to Kaggle (1st stage) üá∞</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90bfdaab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:59.747081Z",
     "iopub.status.busy": "2022-01-10T23:17:59.744233Z",
     "iopub.status.idle": "2022-01-10T23:17:59.749484Z",
     "shell.execute_reply": "2022-01-10T23:17:59.750021Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.406884Z"
    },
    "papermill": {
     "duration": 0.058869,
     "end_time": "2022-01-10T23:17:59.750175",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.691306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pred_df = pd.DataFrame({'Id':test_df.Id, 'Pawpularity':nets_preds.reshape(-1)})\n",
    "# sub_df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/sample_submission.csv')\n",
    "# del sub_df['Pawpularity']\n",
    "# sub_df = sub_df.merge(pred_df, on='Id', how='left')\n",
    "# sub_df.to_csv('submission_nn.csv',index=False)\n",
    "# sub_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f75049",
   "metadata": {
    "papermill": {
     "duration": 0.05008,
     "end_time": "2022-01-10T23:17:59.851230",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.801150",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br><br><br>\n",
    "\n",
    "# <span class=\"title-section w3-xxlarge\" id=\"catboost\"> 2nd Stage Inference (Catboost) üî•</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81fd3e0a",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-01-10T23:17:59.972036Z",
     "iopub.status.busy": "2022-01-10T23:17:59.967387Z",
     "iopub.status.idle": "2022-01-11T02:24:07.151555Z",
     "shell.execute_reply": "2022-01-11T02:24:07.150828Z",
     "shell.execute_reply.started": "2022-01-10T13:17:30.413874Z"
    },
    "papermill": {
     "duration": 11167.249586,
     "end_time": "2022-01-11T02:24:07.151728",
     "exception": false,
     "start_time": "2022-01-10T23:17:59.902142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/pawpularset/Keras_Applications-1.0.8-py3-none-any.whl\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (3.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from Keras-Applications==1.0.8) (1.19.5)\r\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py->Keras-Applications==1.0.8) (1.5.2)\r\n",
      "Installing collected packages: Keras-Applications\r\n",
      "Successfully installed Keras-Applications-1.0.8\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "Processing /kaggle/input/pawpularset/efficientnet-1.1.1-py3-none-any.whl\r\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.1) (1.0.8)\r\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet==1.1.1) (0.18.3)\r\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.19.5)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (3.1.0)\r\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.6.3)\r\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (3.5.1)\r\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.7.3)\r\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2.9.0)\r\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (8.2.0)\r\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (1.2.0)\r\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet==1.1.1) (2021.11.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (4.28.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (3.0.6)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (2.8.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (0.11.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.3.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (21.3)\r\n",
      "Requirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.1.1) (1.5.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet==1.1.1) (1.16.0)\r\n",
      "efficientnet is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n",
      "train_df: (9912, 16)\n",
      "test_df: (8, 14)\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 0 ======================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 23:19:00.189659: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-10 23:19:00.191638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:19:00.192875: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:19:00.193960: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:19:00.196858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:19:00.197987: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:19:00.199152: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-10 23:19:00.200126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13385 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Extracting Features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 23:19:06.159994: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-01-10 23:19:06.193881: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1652] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
      "2022-01-10 23:19:08.334443: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7ff5d40291e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2022-01-10 23:19:08.334548: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "2022-01-10 23:19:08.564424: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2022-01-10 23:19:09.530422: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3/620 [..............................] - ETA: 40s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-10 23:19:16.324939: I tensorflow/compiler/jit/xla_compilation_cache.cc:363] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "620/620 [==============================] - 109s 160ms/step\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4261\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.3804645\ttest: 20.4505548\tbest: 20.4505548 (0)\ttotal: 644ms\tremaining: 10m 43s\n",
      "250:\tlearn: 14.2664586\ttest: 17.3478008\tbest: 17.3397825 (223)\ttotal: 1m 41s\tremaining: 5m 2s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.33978249\n",
      "bestIteration = 223\n",
      "\n",
      "Shrink model to first 224 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3874572\ttest: 20.4186462\tbest: 20.4186462 (0)\ttotal: 523ms\tremaining: 8m 42s\n",
      "250:\tlearn: 14.3211471\ttest: 17.5717897\tbest: 17.5361327 (165)\ttotal: 1m 39s\tremaining: 4m 56s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.53613265\n",
      "bestIteration = 165\n",
      "\n",
      "Shrink model to first 166 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3777028\ttest: 20.4234454\tbest: 20.4234454 (0)\ttotal: 512ms\tremaining: 8m 31s\n",
      "250:\tlearn: 14.3101666\ttest: 17.4009675\tbest: 17.3844533 (197)\ttotal: 1m 38s\tremaining: 4m 52s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.3638208\n",
      "bestIteration = 326\n",
      "\n",
      "Shrink model to first 327 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3960686\ttest: 20.3926260\tbest: 20.3926260 (0)\ttotal: 523ms\tremaining: 8m 42s\n",
      "250:\tlearn: 14.4008284\ttest: 17.3638097\tbest: 17.3223480 (200)\ttotal: 1m 37s\tremaining: 4m 50s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.32234802\n",
      "bestIteration = 200\n",
      "\n",
      "Shrink model to first 201 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.3933495\ttest: 20.3536327\tbest: 20.3536327 (0)\ttotal: 511ms\tremaining: 8m 30s\n",
      "250:\tlearn: 14.3043962\ttest: 17.3345607\tbest: 17.3285278 (244)\ttotal: 1m 36s\tremaining: 4m 49s\n",
      "500:\tlearn: 11.7387031\ttest: 17.3233229\tbest: 17.2889743 (435)\ttotal: 3m 12s\tremaining: 3m 11s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.28897432\n",
      "bestIteration = 435\n",
      "\n",
      "Shrink model to first 436 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.3794253\ttest: 20.3881699\tbest: 20.3881699 (0)\ttotal: 517ms\tremaining: 8m 36s\n",
      "250:\tlearn: 14.2805907\ttest: 17.4401622\tbest: 17.4004617 (219)\ttotal: 1m 37s\tremaining: 4m 52s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.40046166\n",
      "bestIteration = 219\n",
      "\n",
      "Shrink model to first 220 iterations.\n",
      "CatBoost OOF Score: 17.375253323867355\n",
      "Test Predictions Cumulative...\n",
      "[[     251.38]\n",
      " [     258.62]\n",
      " [     254.68]\n",
      " [     250.84]\n",
      " [     260.16]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 1 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 90s 137ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4262\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.3940618\ttest: 20.4018669\tbest: 20.4018669 (0)\ttotal: 527ms\tremaining: 8m 46s\n",
      "250:\tlearn: 14.4466621\ttest: 17.3045450\tbest: 17.2887784 (223)\ttotal: 1m 38s\tremaining: 4m 54s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.28877844\n",
      "bestIteration = 223\n",
      "\n",
      "Shrink model to first 224 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3840332\ttest: 20.4696665\tbest: 20.4696665 (0)\ttotal: 516ms\tremaining: 8m 35s\n",
      "250:\tlearn: 14.4346555\ttest: 17.2150846\tbest: 17.2148657 (249)\ttotal: 1m 39s\tremaining: 4m 57s\n",
      "500:\tlearn: 11.8188234\ttest: 17.1743633\tbest: 17.1740511 (499)\ttotal: 3m 17s\tremaining: 3m 17s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.17236265\n",
      "bestIteration = 501\n",
      "\n",
      "Shrink model to first 502 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3946873\ttest: 20.3769538\tbest: 20.3769538 (0)\ttotal: 515ms\tremaining: 8m 34s\n",
      "250:\tlearn: 14.4098523\ttest: 17.6036367\tbest: 17.5631612 (202)\ttotal: 1m 39s\tremaining: 4m 56s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.56316124\n",
      "bestIteration = 202\n",
      "\n",
      "Shrink model to first 203 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3688169\ttest: 20.5393971\tbest: 20.5393971 (0)\ttotal: 520ms\tremaining: 8m 39s\n",
      "250:\tlearn: 14.3814885\ttest: 17.3218822\tbest: 17.3121109 (227)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.28649545\n",
      "bestIteration = 321\n",
      "\n",
      "Shrink model to first 322 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.4142652\ttest: 20.3112718\tbest: 20.3112718 (0)\ttotal: 522ms\tremaining: 8m 41s\n",
      "250:\tlearn: 14.3211294\ttest: 17.5434409\tbest: 17.5302921 (207)\ttotal: 1m 39s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.5215914\n",
      "bestIteration = 316\n",
      "\n",
      "Shrink model to first 317 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.3995273\ttest: 20.3650520\tbest: 20.3650520 (0)\ttotal: 599ms\tremaining: 9m 58s\n",
      "250:\tlearn: 14.4047374\ttest: 17.4577996\tbest: 17.4446449 (231)\ttotal: 1m 41s\tremaining: 5m 1s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.44464491\n",
      "bestIteration = 231\n",
      "\n",
      "Shrink model to first 232 iterations.\n",
      "CatBoost OOF Score: 17.37950568065062\n",
      "Test Predictions Cumulative...\n",
      "[[      498.7]\n",
      " [     524.68]\n",
      " [     505.05]\n",
      " [     516.56]\n",
      " [     532.11]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 2 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 94s 143ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4263\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.3733332\ttest: 20.3881049\tbest: 20.3881049 (0)\ttotal: 514ms\tremaining: 8m 33s\n",
      "250:\tlearn: 14.3577951\ttest: 17.6538744\tbest: 17.6520148 (247)\ttotal: 1m 39s\tremaining: 4m 57s\n",
      "500:\tlearn: 11.8016904\ttest: 17.6547234\tbest: 17.6363092 (404)\ttotal: 3m 18s\tremaining: 3m 17s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.63630916\n",
      "bestIteration = 404\n",
      "\n",
      "Shrink model to first 405 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3547622\ttest: 20.4720243\tbest: 20.4720243 (0)\ttotal: 520ms\tremaining: 8m 39s\n",
      "250:\tlearn: 14.3141392\ttest: 17.3615415\tbest: 17.3615415 (250)\ttotal: 1m 39s\tremaining: 4m 57s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.31853664\n",
      "bestIteration = 313\n",
      "\n",
      "Shrink model to first 314 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3861960\ttest: 20.3513783\tbest: 20.3513783 (0)\ttotal: 523ms\tremaining: 8m 42s\n",
      "250:\tlearn: 14.2745955\ttest: 17.5022053\tbest: 17.4862340 (222)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.48623402\n",
      "bestIteration = 222\n",
      "\n",
      "Shrink model to first 223 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3706405\ttest: 20.4649030\tbest: 20.4649030 (0)\ttotal: 527ms\tremaining: 8m 46s\n",
      "250:\tlearn: 14.4740207\ttest: 17.0028446\tbest: 16.9965491 (249)\ttotal: 1m 39s\tremaining: 4m 57s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 16.98318693\n",
      "bestIteration = 291\n",
      "\n",
      "Shrink model to first 292 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.4050556\ttest: 20.2513001\tbest: 20.2513001 (0)\ttotal: 527ms\tremaining: 8m 46s\n",
      "250:\tlearn: 14.3754393\ttest: 17.4647439\tbest: 17.4435580 (178)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.44355804\n",
      "bestIteration = 178\n",
      "\n",
      "Shrink model to first 179 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.3774940\ttest: 20.3999253\tbest: 20.3999253 (0)\ttotal: 516ms\tremaining: 8m 35s\n",
      "250:\tlearn: 14.3663198\ttest: 17.0974008\tbest: 17.0625935 (192)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.06259346\n",
      "bestIteration = 192\n",
      "\n",
      "Shrink model to first 193 iterations.\n",
      "CatBoost OOF Score: 17.321736374193986\n",
      "Test Predictions Cumulative...\n",
      "[[     757.22]\n",
      " [     789.99]\n",
      " [     768.35]\n",
      " [     776.03]\n",
      " [     801.86]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 3 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 94s 142ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4264\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.3824145\ttest: 20.4792239\tbest: 20.4792239 (0)\ttotal: 936ms\tremaining: 15m 35s\n",
      "250:\tlearn: 14.5817288\ttest: 17.0130658\tbest: 17.0114828 (248)\ttotal: 1m 40s\tremaining: 5m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 16.98728885\n",
      "bestIteration = 270\n",
      "\n",
      "Shrink model to first 271 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3984423\ttest: 20.4726854\tbest: 20.4726854 (0)\ttotal: 535ms\tremaining: 8m 54s\n",
      "250:\tlearn: 14.5134765\ttest: 17.9318403\tbest: 17.9273481 (244)\ttotal: 1m 39s\tremaining: 4m 57s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.92111906\n",
      "bestIteration = 260\n",
      "\n",
      "Shrink model to first 261 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3882695\ttest: 20.3465708\tbest: 20.3465708 (0)\ttotal: 708ms\tremaining: 11m 47s\n",
      "250:\tlearn: 14.4857782\ttest: 17.7797193\tbest: 17.7642760 (237)\ttotal: 1m 40s\tremaining: 5m 1s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.76427596\n",
      "bestIteration = 237\n",
      "\n",
      "Shrink model to first 238 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3969179\ttest: 20.4046343\tbest: 20.4046343 (0)\ttotal: 530ms\tremaining: 8m 49s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.41645786\n",
      "bestIteration = 116\n",
      "\n",
      "Shrink model to first 117 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.4106052\ttest: 20.3467397\tbest: 20.3467397 (0)\ttotal: 526ms\tremaining: 8m 45s\n",
      "250:\tlearn: 14.4772708\ttest: 17.4280123\tbest: 17.4222625 (240)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.42226247\n",
      "bestIteration = 240\n",
      "\n",
      "Shrink model to first 241 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.3871210\ttest: 20.4368270\tbest: 20.4368270 (0)\ttotal: 533ms\tremaining: 8m 52s\n",
      "250:\tlearn: 14.4856418\ttest: 17.4057149\tbest: 17.3998455 (242)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.39680895\n",
      "bestIteration = 254\n",
      "\n",
      "Shrink model to first 255 iterations.\n",
      "CatBoost OOF Score: 17.48470219431948\n",
      "Test Predictions Cumulative...\n",
      "[[     1027.5]\n",
      " [     1053.5]\n",
      " [     1030.9]\n",
      " [     1043.2]\n",
      " [       1071]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 4 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 96s 146ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4265\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.3361192\ttest: 20.4843156\tbest: 20.4843156 (0)\ttotal: 535ms\tremaining: 8m 54s\n",
      "250:\tlearn: 14.3361171\ttest: 17.8367827\tbest: 17.8349409 (249)\ttotal: 1m 40s\tremaining: 5m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.80759635\n",
      "bestIteration = 338\n",
      "\n",
      "Shrink model to first 339 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3414717\ttest: 20.4214995\tbest: 20.4214995 (0)\ttotal: 530ms\tremaining: 8m 49s\n",
      "250:\tlearn: 14.3835106\ttest: 17.6229576\tbest: 17.6156671 (237)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.61566714\n",
      "bestIteration = 237\n",
      "\n",
      "Shrink model to first 238 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3791138\ttest: 20.3071157\tbest: 20.3071157 (0)\ttotal: 868ms\tremaining: 14m 27s\n",
      "250:\tlearn: 14.4096575\ttest: 17.3955144\tbest: 17.3955144 (250)\ttotal: 1m 41s\tremaining: 5m 2s\n",
      "500:\tlearn: 11.8868369\ttest: 17.3543172\tbest: 17.3481249 (457)\ttotal: 3m 20s\tremaining: 3m 19s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.34812491\n",
      "bestIteration = 457\n",
      "\n",
      "Shrink model to first 458 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3573682\ttest: 20.3575581\tbest: 20.3575581 (0)\ttotal: 529ms\tremaining: 8m 48s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.29636819\n",
      "bestIteration = 131\n",
      "\n",
      "Shrink model to first 132 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.3639994\ttest: 20.3827750\tbest: 20.3827750 (0)\ttotal: 514ms\tremaining: 8m 33s\n",
      "250:\tlearn: 14.3291233\ttest: 17.5282330\tbest: 17.4974225 (157)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.49742246\n",
      "bestIteration = 157\n",
      "\n",
      "Shrink model to first 158 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.4030790\ttest: 20.3185914\tbest: 20.3185914 (0)\ttotal: 540ms\tremaining: 8m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 16.82924265\n",
      "bestIteration = 108\n",
      "\n",
      "Shrink model to first 109 iterations.\n",
      "CatBoost OOF Score: 17.39907028418868\n",
      "Test Predictions Cumulative...\n",
      "[[     1267.6]\n",
      " [     1304.2]\n",
      " [     1271.3]\n",
      " [     1287.2]\n",
      " [     1300.4]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 5 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 96s 146ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4266\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.4043402\ttest: 20.3671879\tbest: 20.3671879 (0)\ttotal: 581ms\tremaining: 9m 39s\n",
      "250:\tlearn: 14.3686757\ttest: 17.1724965\tbest: 17.1521355 (242)\ttotal: 1m 40s\tremaining: 5m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.15213547\n",
      "bestIteration = 242\n",
      "\n",
      "Shrink model to first 243 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3896741\ttest: 20.4689944\tbest: 20.4689944 (0)\ttotal: 522ms\tremaining: 8m 41s\n",
      "250:\tlearn: 14.3668309\ttest: 17.6411008\tbest: 17.6159867 (225)\ttotal: 1m 39s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.61598668\n",
      "bestIteration = 225\n",
      "\n",
      "Shrink model to first 226 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3805354\ttest: 20.4280878\tbest: 20.4280878 (0)\ttotal: 518ms\tremaining: 8m 37s\n",
      "250:\tlearn: 14.4491783\ttest: 17.0558231\tbest: 17.0553456 (248)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "500:\tlearn: 11.8223653\ttest: 16.9913178\tbest: 16.9868137 (498)\ttotal: 3m 20s\tremaining: 3m 19s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 16.98315151\n",
      "bestIteration = 552\n",
      "\n",
      "Shrink model to first 553 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3840394\ttest: 20.4618718\tbest: 20.4618718 (0)\ttotal: 753ms\tremaining: 12m 31s\n",
      "250:\tlearn: 14.3004235\ttest: 17.5820295\tbest: 17.5802277 (247)\ttotal: 1m 40s\tremaining: 5m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.56564428\n",
      "bestIteration = 287\n",
      "\n",
      "Shrink model to first 288 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.4046734\ttest: 20.3637782\tbest: 20.3637782 (0)\ttotal: 533ms\tremaining: 8m 52s\n",
      "250:\tlearn: 14.3250856\ttest: 17.5892036\tbest: 17.5520248 (206)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.5520248\n",
      "bestIteration = 206\n",
      "\n",
      "Shrink model to first 207 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.4002642\ttest: 20.3960262\tbest: 20.3960262 (0)\ttotal: 526ms\tremaining: 8m 45s\n",
      "250:\tlearn: 14.3943825\ttest: 17.5746353\tbest: 17.5660196 (241)\ttotal: 1m 39s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.55841428\n",
      "bestIteration = 267\n",
      "\n",
      "Shrink model to first 268 iterations.\n",
      "CatBoost OOF Score: 17.404559504676108\n",
      "Test Predictions Cumulative...\n",
      "[[     1489.6]\n",
      " [       1526]\n",
      " [     1484.1]\n",
      " [     1501.8]\n",
      " [     1519.7]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 6 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 96s 145ms/step\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4267\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.4036329\ttest: 20.4511376\tbest: 20.4511376 (0)\ttotal: 527ms\tremaining: 8m 46s\n",
      "250:\tlearn: 14.4338008\ttest: 17.3128190\tbest: 17.2956229 (232)\ttotal: 1m 39s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.29562287\n",
      "bestIteration = 232\n",
      "\n",
      "Shrink model to first 233 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.4029249\ttest: 20.4320549\tbest: 20.4320549 (0)\ttotal: 525ms\tremaining: 8m 44s\n",
      "250:\tlearn: 14.4560093\ttest: 17.2537876\tbest: 17.2287743 (243)\ttotal: 1m 39s\tremaining: 4m 56s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.2074752\n",
      "bestIteration = 391\n",
      "\n",
      "Shrink model to first 392 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3738830\ttest: 20.5932431\tbest: 20.5932431 (0)\ttotal: 681ms\tremaining: 11m 20s\n",
      "250:\tlearn: 14.4119851\ttest: 17.5174229\tbest: 17.5044458 (238)\ttotal: 1m 39s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.50444584\n",
      "bestIteration = 238\n",
      "\n",
      "Shrink model to first 239 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.4015256\ttest: 20.4378099\tbest: 20.4378099 (0)\ttotal: 536ms\tremaining: 8m 54s\n",
      "250:\tlearn: 14.4317888\ttest: 17.5572578\tbest: 17.5544862 (222)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "500:\tlearn: 11.8176784\ttest: 17.4871044\tbest: 17.4818700 (467)\ttotal: 3m 20s\tremaining: 3m 19s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.4558948\n",
      "bestIteration = 592\n",
      "\n",
      "Shrink model to first 593 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.4204490\ttest: 20.3507745\tbest: 20.3507745 (0)\ttotal: 531ms\tremaining: 8m 50s\n",
      "250:\tlearn: 14.4569229\ttest: 17.7505202\tbest: 17.7505202 (250)\ttotal: 1m 39s\tremaining: 4m 57s\n",
      "500:\tlearn: 11.8266246\ttest: 17.6917251\tbest: 17.6808839 (457)\ttotal: 3m 19s\tremaining: 3m 18s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.68088391\n",
      "bestIteration = 457\n",
      "\n",
      "Shrink model to first 458 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.4242246\ttest: 20.2857139\tbest: 20.2857139 (0)\ttotal: 541ms\tremaining: 9m\n",
      "250:\tlearn: 14.5563228\ttest: 16.9546948\tbest: 16.9423423 (216)\ttotal: 1m 38s\tremaining: 4m 54s\n",
      "500:\tlearn: 11.9774233\ttest: 16.8712466\tbest: 16.8641364 (495)\ttotal: 3m 18s\tremaining: 3m 17s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 16.86413642\n",
      "bestIteration = 495\n",
      "\n",
      "Shrink model to first 496 iterations.\n",
      "CatBoost OOF Score: 17.334743172358248\n",
      "Test Predictions Cumulative...\n",
      "[[     1750.8]\n",
      " [     1783.9]\n",
      " [       1738]\n",
      " [     1761.2]\n",
      " [     1780.4]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 7 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 94s 142ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4268\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.3927354\ttest: 20.3559064\tbest: 20.3559064 (0)\ttotal: 514ms\tremaining: 8m 33s\n",
      "250:\tlearn: 14.4071627\ttest: 17.7121358\tbest: 17.7018476 (225)\ttotal: 1m 39s\tremaining: 4m 55s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.67807218\n",
      "bestIteration = 343\n",
      "\n",
      "Shrink model to first 344 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3949914\ttest: 20.4269189\tbest: 20.4269189 (0)\ttotal: 539ms\tremaining: 8m 58s\n",
      "250:\tlearn: 14.4902190\ttest: 17.1767403\tbest: 17.1511019 (202)\ttotal: 1m 39s\tremaining: 4m 56s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.15110186\n",
      "bestIteration = 202\n",
      "\n",
      "Shrink model to first 203 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3783002\ttest: 20.4115357\tbest: 20.4115357 (0)\ttotal: 524ms\tremaining: 8m 43s\n",
      "250:\tlearn: 14.4294713\ttest: 17.5421192\tbest: 17.5383298 (247)\ttotal: 1m 39s\tremaining: 4m 55s\n",
      "500:\tlearn: 11.8311477\ttest: 17.4874986\tbest: 17.4743161 (472)\ttotal: 3m 19s\tremaining: 3m 18s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.46766057\n",
      "bestIteration = 565\n",
      "\n",
      "Shrink model to first 566 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3761603\ttest: 20.4889564\tbest: 20.4889564 (0)\ttotal: 532ms\tremaining: 8m 50s\n",
      "250:\tlearn: 14.4191684\ttest: 18.0273161\tbest: 18.0001253 (221)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 18.0001253\n",
      "bestIteration = 221\n",
      "\n",
      "Shrink model to first 222 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.3916838\ttest: 20.3600180\tbest: 20.3600180 (0)\ttotal: 518ms\tremaining: 8m 37s\n",
      "250:\tlearn: 14.5247057\ttest: 16.9130944\tbest: 16.9119863 (236)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "500:\tlearn: 11.9508342\ttest: 16.8302702\tbest: 16.8103886 (467)\ttotal: 3m 21s\tremaining: 3m 20s\n",
      "750:\tlearn: 9.9830983\ttest: 16.7892379\tbest: 16.7787907 (736)\ttotal: 5m 2s\tremaining: 1m 40s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 16.77879073\n",
      "bestIteration = 736\n",
      "\n",
      "Shrink model to first 737 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.4102009\ttest: 20.3654602\tbest: 20.3654602 (0)\ttotal: 719ms\tremaining: 11m 58s\n",
      "250:\tlearn: 14.4697213\ttest: 17.5386330\tbest: 17.5348007 (223)\ttotal: 1m 41s\tremaining: 5m 2s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.51995662\n",
      "bestIteration = 283\n",
      "\n",
      "Shrink model to first 284 iterations.\n",
      "CatBoost OOF Score: 17.432617876479693\n",
      "Test Predictions Cumulative...\n",
      "[[     1998.1]\n",
      " [     2040.6]\n",
      " [     1997.3]\n",
      " [     2022.2]\n",
      " [     2038.3]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 8 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 96s 146ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4269\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.4089533\ttest: 20.3980768\tbest: 20.3980768 (0)\ttotal: 840ms\tremaining: 13m 59s\n",
      "250:\tlearn: 14.5928031\ttest: 17.4380982\tbest: 17.4296400 (248)\ttotal: 1m 40s\tremaining: 5m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.39395644\n",
      "bestIteration = 399\n",
      "\n",
      "Shrink model to first 400 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3898379\ttest: 20.4400053\tbest: 20.4400053 (0)\ttotal: 726ms\tremaining: 12m 5s\n",
      "250:\tlearn: 14.5680914\ttest: 17.7509260\tbest: 17.7172089 (153)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.71720886\n",
      "bestIteration = 153\n",
      "\n",
      "Shrink model to first 154 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3829717\ttest: 20.4740046\tbest: 20.4740046 (0)\ttotal: 519ms\tremaining: 8m 38s\n",
      "250:\tlearn: 14.4432418\ttest: 18.3739504\tbest: 18.3581912 (204)\ttotal: 1m 40s\tremaining: 4m 58s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 18.34653396\n",
      "bestIteration = 342\n",
      "\n",
      "Shrink model to first 343 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3988045\ttest: 20.4370372\tbest: 20.4370372 (0)\ttotal: 791ms\tremaining: 13m 9s\n",
      "250:\tlearn: 14.5789570\ttest: 17.2437034\tbest: 17.2437034 (250)\ttotal: 1m 40s\tremaining: 5m\n",
      "500:\tlearn: 11.9583365\ttest: 17.1650034\tbest: 17.1481985 (422)\ttotal: 3m 21s\tremaining: 3m 20s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.14819845\n",
      "bestIteration = 422\n",
      "\n",
      "Shrink model to first 423 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.4082524\ttest: 20.3143832\tbest: 20.3143832 (0)\ttotal: 524ms\tremaining: 8m 43s\n",
      "250:\tlearn: 14.5403587\ttest: 17.4448016\tbest: 17.4259740 (217)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.42597405\n",
      "bestIteration = 217\n",
      "\n",
      "Shrink model to first 218 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.3992309\ttest: 20.4244615\tbest: 20.4244615 (0)\ttotal: 750ms\tremaining: 12m 29s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.19514444\n",
      "bestIteration = 127\n",
      "\n",
      "Shrink model to first 128 iterations.\n",
      "CatBoost OOF Score: 17.537836032699854\n",
      "Test Predictions Cumulative...\n",
      "[[     2250.3]\n",
      " [     2291.2]\n",
      " [     2247.2]\n",
      " [     2274.3]\n",
      " [     2291.7]]\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "===== Run for Feature Model 9 ======================================================================\n",
      "\n",
      "\n",
      "===== Extracting Features\n",
      "620/620 [==============================] - 97s 147ms/step\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "\n",
      "===== Feature Set Shapes\n",
      "Train Feature Set Shape: (9912, 1408)\n",
      "Test Feature Set Shape: (8, 1408)\n",
      "\n",
      "===== Running CatBoost - SEED 4270\n",
      "\n",
      "===== CatBoost Fold 0 ===============================================================================================\n",
      "0:\tlearn: 20.3957154\ttest: 20.3655718\tbest: 20.3655718 (0)\ttotal: 728ms\tremaining: 12m 6s\n",
      "250:\tlearn: 14.5043112\ttest: 17.2768946\tbest: 17.2765366 (249)\ttotal: 1m 40s\tremaining: 5m\n",
      "500:\tlearn: 11.8515238\ttest: 17.2353751\tbest: 17.2336401 (467)\ttotal: 3m 21s\tremaining: 3m 20s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.1935275\n",
      "bestIteration = 619\n",
      "\n",
      "Shrink model to first 620 iterations.\n",
      "\n",
      "===== CatBoost Fold 1 ===============================================================================================\n",
      "0:\tlearn: 20.3707138\ttest: 20.4982986\tbest: 20.4982986 (0)\ttotal: 875ms\tremaining: 14m 33s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.74428473\n",
      "bestIteration = 133\n",
      "\n",
      "Shrink model to first 134 iterations.\n",
      "\n",
      "===== CatBoost Fold 2 ===============================================================================================\n",
      "0:\tlearn: 20.3818788\ttest: 20.4314029\tbest: 20.4314029 (0)\ttotal: 825ms\tremaining: 13m 44s\n",
      "250:\tlearn: 14.4367844\ttest: 17.6369758\tbest: 17.6291355 (239)\ttotal: 1m 40s\tremaining: 4m 59s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.61363433\n",
      "bestIteration = 323\n",
      "\n",
      "Shrink model to first 324 iterations.\n",
      "\n",
      "===== CatBoost Fold 3 ===============================================================================================\n",
      "0:\tlearn: 20.3715386\ttest: 20.4127644\tbest: 20.4127644 (0)\ttotal: 828ms\tremaining: 13m 47s\n",
      "250:\tlearn: 14.5139483\ttest: 17.1520872\tbest: 17.1476541 (247)\ttotal: 1m 40s\tremaining: 5m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.1455662\n",
      "bestIteration = 279\n",
      "\n",
      "Shrink model to first 280 iterations.\n",
      "\n",
      "===== CatBoost Fold 4 ===============================================================================================\n",
      "0:\tlearn: 20.3878786\ttest: 20.4280970\tbest: 20.4280970 (0)\ttotal: 820ms\tremaining: 13m 39s\n",
      "250:\tlearn: 14.3708850\ttest: 17.8256765\tbest: 17.8199244 (245)\ttotal: 1m 41s\tremaining: 5m 2s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 17.78991261\n",
      "bestIteration = 367\n",
      "\n",
      "Shrink model to first 368 iterations.\n",
      "\n",
      "===== CatBoost Fold 5 ===============================================================================================\n",
      "0:\tlearn: 20.4174840\ttest: 20.2714524\tbest: 20.2714524 (0)\ttotal: 1.16s\tremaining: 19m 15s\n",
      "250:\tlearn: 14.5604600\ttest: 17.0028584\tbest: 16.9973836 (235)\ttotal: 1m 41s\tremaining: 5m 2s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 16.93090102\n",
      "bestIteration = 399\n",
      "\n",
      "Shrink model to first 400 iterations.\n",
      "CatBoost OOF Score: 17.40297106382791\n",
      "Test Predictions Cumulative...\n",
      "[[     2488.1]\n",
      " [     2541.1]\n",
      " [     2488.5]\n",
      " [       2510]\n",
      " [     2534.2]]\n",
      "Final OOF RMSE Score for all feature models: 17.407299550726197\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Install libraries\n",
    "!pip install '../input/pawpularset/Keras_Applications-1.0.8-py3-none-any.whl'\n",
    "!pip install '../input/pawpularset/efficientnet-1.1.1-py3-none-any.whl'\n",
    "\n",
    "# Import libraries\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import efficientnet.tfkeras as efn\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 384\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "Q = 30\n",
    "EPOCHS = 10\n",
    "FOLDS = 6\n",
    "FEATURE_FOLDS = 10\n",
    "SEED = 4261\n",
    "VERBOSE = 1\n",
    "LR = 0.000005\n",
    "TRAIN_CATB =True #False\n",
    "\n",
    "# Logic...\n",
    "TRAIN_FEATURE_MODEL = False\n",
    "\n",
    "# Folders\n",
    "DATA_DIR = '../input/petfinder-pawpularity-score/'\n",
    "TRAIN_DIR = DATA_DIR + 'train/'\n",
    "TEST_DIR = DATA_DIR + 'test/'\n",
    "\n",
    "\n",
    "# Configure Strategy. Assume TPU...if not set default for GPU/CPU\n",
    "tpu = None\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except ValueError:\n",
    "    # Enable XLA\n",
    "    tf.config.optimizer.set_jit(enabled = \"autoclustering\")\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    \n",
    "# Set Auto Tune\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE   \n",
    "\n",
    "\n",
    "# Load Train Data\n",
    "train_df = pd.read_csv(f'{DATA_DIR}train.csv')\n",
    "train_df['Id'] = train_df['Id'].apply(lambda x: f'{TRAIN_DIR}{x}.jpg')\n",
    "\n",
    "# Set a specific label to be able to perform stratification\n",
    "train_df['stratify_label'] = pd.qcut(train_df['Pawpularity'], q = Q, labels = range(Q))\n",
    "\n",
    "# Label value to be used for feature model 'classification' training.\n",
    "train_df['target_value'] = train_df['Pawpularity'] / 100.\n",
    "\n",
    "# Summary\n",
    "print(f'train_df: {train_df.shape}')\n",
    "train_df.head()\n",
    "\n",
    "\n",
    "# Load Test Data\n",
    "test_df = pd.read_csv(f'{DATA_DIR}test.csv')\n",
    "test_df['Id'] = test_df['Id'].apply(lambda x: f'{TEST_DIR}{x}.jpg')\n",
    "test_df['Pawpularity'] = 0\n",
    "\n",
    "# Summary\n",
    "print(f'test_df: {test_df.shape}')\n",
    "test_df.head()\n",
    "\n",
    "\n",
    "def build_augmenter(is_labelled):\n",
    "    def augment(img):\n",
    "        # Only use basic augmentations...too much augmentation hurts performance\n",
    "        img = tf.image.random_flip_left_right(img)\n",
    "        img = tf.image.random_flip_up_down(img)\n",
    "        img = tf.image.random_saturation(img, 0.95, 1.05)\n",
    "        img = tf.image.random_brightness(img, 0.05)\n",
    "        img = tf.image.random_contrast(img, 0.95, 1.05)\n",
    "        img = tf.image.random_hue(img, 0.05)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def augment_with_labels(img, label):\n",
    "        return augment(img), label\n",
    "    \n",
    "    return augment_with_labels if is_labelled else augment\n",
    "\n",
    "def build_decoder(is_labelled):\n",
    "    def decode(path):\n",
    "        # Read Image\n",
    "        file_bytes = tf.io.read_file(path)\n",
    "        img = tf.image.decode_jpeg(file_bytes, channels = CHANNELS)\n",
    "        \n",
    "        # Normalize and Resize\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def decode_with_labels(path, label):\n",
    "        return decode(path), label\n",
    "    \n",
    "    return decode_with_labels if is_labelled else decode\n",
    "\n",
    "def create_dataset(df, batch_size = 32, is_labelled = False, augment = False, repeat = False, shuffle = False):\n",
    "    decode_fn = build_decoder(is_labelled)\n",
    "    augmenter_fn = build_augmenter(is_labelled)\n",
    "    \n",
    "    # Create Dataset\n",
    "    if is_labelled:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values, df['target_value'].values))\n",
    "    else:\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((df['Id'].values))\n",
    "    dataset = dataset.map(decode_fn, num_parallel_calls = AUTOTUNE)\n",
    "    dataset = dataset.map(augmenter_fn, num_parallel_calls = AUTOTUNE) if augment else dataset\n",
    "    dataset = dataset.repeat() if repeat else dataset\n",
    "    dataset = dataset.shuffle(1024, reshuffle_each_iteration = True) if shuffle else dataset\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "    \n",
    "# Set Callbacks\n",
    "def model_checkpoint(fold):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(f'feature_model_{fold}.h5',\n",
    "                                              verbose = 1, \n",
    "                                              monitor = 'val_rmse', \n",
    "                                              mode = 'min', \n",
    "                                              save_weights_only = True,\n",
    "                                              save_best_only = True)\n",
    "\n",
    "def unfreeze_model(model):\n",
    "    # Unfreeze layers while leaving BatchNorm layers frozen\n",
    "    for layer in model.layers:\n",
    "        if not isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "            layer.trainable = True\n",
    "        else:\n",
    "            layer.trainable = False\n",
    "\n",
    "def create_model(): \n",
    "    # Create and Compile Model and show Summary\n",
    "    effnet_model = efn.EfficientNetB2(include_top = False, \n",
    "                                      classes = None, \n",
    "                                      input_shape = (IMG_SIZE, IMG_SIZE, CHANNELS), \n",
    "                                      weights = '../input/pawpularset/efficientnet-b2_noisy-student_notop.h5', \n",
    "                                      pooling = 'avg')\n",
    "\n",
    "    # Set all layers to Trainable except BN layers\n",
    "    unfreeze_model(effnet_model)\n",
    "    \n",
    "    X = tf.keras.layers.Dropout(0.25)(effnet_model.output)\n",
    "    output = tf.keras.layers.Dense(1, activation = 'sigmoid')(X)\n",
    "    \n",
    "    # Create Final Model\n",
    "    model = tf.keras.Model(inputs = effnet_model.input, outputs = output)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = LR), \n",
    "                  loss = tf.keras.losses.BinaryCrossentropy(), \n",
    "                  metrics = [tf.keras.metrics.RootMeanSquaredError('rmse')])        \n",
    "    \n",
    "    return model\n",
    "\n",
    "## EfficientNet Feature Model Training\n",
    "\n",
    "\n",
    "if TRAIN_FEATURE_MODEL:\n",
    "    # OOF RMSE Placeholder\n",
    "    all_val_rmse = []\n",
    "\n",
    "    # Stratified Training\n",
    "    kfold = StratifiedKFold(n_splits = FEATURE_FOLDS, shuffle = True, random_state = SEED)\n",
    "    for fold, (train_index, val_index) in enumerate(kfold.split(train_df.index, train_df['stratify_label'])):\n",
    "        print(f'\\n===== Fold {fold}\\n')\n",
    "\n",
    "        # Pre model.fit cleanup\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        # Create Model\n",
    "        model = create_model()\n",
    "\n",
    "        # Create TF Datasets\n",
    "        trn = train_df.iloc[train_index]\n",
    "        val = train_df.iloc[val_index]\n",
    "        training_dataset = create_dataset(trn, batch_size = BATCH_SIZE, is_labelled = True, augment = True, repeat = True, shuffle = True)\n",
    "        validation_dataset = create_dataset(val, batch_size = BATCH_SIZE, is_labelled = True, augment = False, repeat = True, shuffle = False)\n",
    "\n",
    "        # Fit Model\n",
    "        history = model.fit(training_dataset,\n",
    "                            epochs = EPOCHS,\n",
    "                            steps_per_epoch = trn.shape[0] // BATCH_SIZE,\n",
    "                            validation_steps = val.shape[0] // BATCH_SIZE,\n",
    "                            callbacks = [model_checkpoint(fold)],\n",
    "                            validation_data = validation_dataset,\n",
    "                            verbose = 1)   \n",
    "\n",
    "        # Validation Information\n",
    "        best_val_rmse = min(history.history['val_rmse'])\n",
    "        all_val_rmse.append(best_val_rmse)\n",
    "        print(f'\\nValidation RMSE: {best_val_rmse}\\n')\n",
    "\n",
    "    # Summary\n",
    "    print(f'Final Mean RMSE for {FEATURE_FOLDS} Fold CV Training: {np.mean(all_val_rmse)}')\n",
    "    \n",
    "    \n",
    "# Placeholders\n",
    "preds_final = np.zeros((test_df.shape[0], 1))\n",
    "all_oof_score = []\n",
    "\n",
    "catb_preds = []\n",
    "# Stratification and Label values\n",
    "Y_strat = train_df['stratify_label'].values\n",
    "Y_pawpularity = train_df['Pawpularity'].values\n",
    "# Loop through all Feature Extraction Models\n",
    "for fold_index in range(FEATURE_FOLDS):\n",
    "    print('\\n\\n====================================================================================================')\n",
    "    print(f'===== Run for Feature Model {fold_index} ======================================================================\\n')\n",
    "\n",
    "    # Pre model.fit cleanup\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # Create Model\n",
    "    model = create_model()\n",
    "    \n",
    "    # Load Weights...Use the provided weight files...or modify for your own set.\n",
    "    model.load_weights(f'../input/pawpularset/feature_model_{fold_index}.h5')\n",
    "    \n",
    "    # Strip Last layers to be able to extract features\n",
    "    model = tf.keras.Model(inputs = model.input, outputs = model.layers[-3].output)\n",
    "    \n",
    "    # Summary...only on first load\n",
    "    #if fold_index == 0: print(model.summary())        \n",
    "        \n",
    "    # Feature Extraction\n",
    "    print('\\n===== Extracting Features')\n",
    "    cb_train_set = create_dataset(train_df, batch_size = BATCH_SIZE, is_labelled = True, augment = False, repeat = False, shuffle = False)\n",
    "    cb_test_set = create_dataset(test_df, batch_size = BATCH_SIZE, is_labelled = False, augment = False, repeat = False, shuffle = False)\n",
    "    cb_train_features = model.predict(cb_train_set, verbose = VERBOSE)\n",
    "    cb_test_features = model.predict(cb_test_set, verbose = VERBOSE)\n",
    "    \n",
    "    print('\\n===== Feature Set Shapes')\n",
    "    print(f'Train Feature Set Shape: {cb_train_features.shape}')\n",
    "    print(f'Test Feature Set Shape: {cb_test_features.shape}')\n",
    "    \n",
    "    # Stratified Training for CatBoost\n",
    "    print(f'\\n===== Running CatBoost - SEED {SEED}')\n",
    "    \n",
    "    # Placeholders\n",
    "    oof_score = 0\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits = FOLDS, shuffle = True, random_state = SEED)\n",
    "    for idx, (train, val) in enumerate(kfold.split(cb_train_features, Y_strat)):\n",
    "        print(f'\\n===== CatBoost Fold {idx} ===============================================================================================')\n",
    "\n",
    "        train_x, train_y = cb_train_features[train], Y_pawpularity[train]\n",
    "        val_x, val_y = cb_train_features[val], Y_pawpularity[val]\n",
    "        \n",
    "        # Set CatBoost Parameters\n",
    "        cb_params = {'loss_function' : 'RMSE',\n",
    "                     'eval_metric' : 'RMSE',\n",
    "                     'iterations' : 1000,\n",
    "                     'grow_policy' : 'SymmetricTree',\n",
    "                     'depth' : 6,\n",
    "                     'l2_leaf_reg' : 2.0,\n",
    "                     'random_strength' : 1.0,\n",
    "                     'learning_rate' : 0.05,\n",
    "                     'task_type' : 'CPU',\n",
    "                     'devices' : '0',\n",
    "                     'verbose' : 0}\n",
    "        \n",
    "        # Create and Fit CatBoost Model\n",
    "        cb_model = CatBoostRegressor(**cb_params)\n",
    "        if TRAIN_CATB:\n",
    "            cb_model.fit(train_x, train_y, eval_set = [(val_x, val_y)], early_stopping_rounds = 100, verbose = 250)\n",
    "            cb_model.save_model(str(fold_index) + '_' + str(idx) + '_meta_catboost_b2_v2.pkl', format = \"cbm\")\n",
    "        else: cb_model.load_model('../input/effnet-b2-feature-models-catboost-2-top/' + str(fold_index) + '_' + str(idx) + '_meta_catboost_b2_v2.pkl', format = \"cbm\")\n",
    "        catb_preds.append( (np.array([cb_model.predict(cb_test_features)]).T).ravel() )            \n",
    "        y_pred = cb_model.predict(val_x)\n",
    "        preds_final += np.array([cb_model.predict(cb_test_features)]).T\n",
    "\n",
    "        # Update OOF Score\n",
    "        oof_score += np.sqrt(mean_squared_error(val_y, y_pred))        \n",
    "\n",
    "        # Cleanup\n",
    "        del cb_model, y_pred\n",
    "        del train_x, train_y\n",
    "        del val_x, val_y\n",
    "        gc.collect()   \n",
    "    \n",
    "    # OOF Score for CatBoost run\n",
    "    oof_score /= FOLDS\n",
    "    all_oof_score.append(oof_score)\n",
    "    print(f'CatBoost OOF Score: {oof_score}')\n",
    "    print('Test Predictions Cumulative...')\n",
    "    print(preds_final[:5])\n",
    "    \n",
    "    # Increase to improve randomness on the next feature model run\n",
    "    SEED += 1\n",
    "    \n",
    "# Final OOF score for All Feature Models\n",
    "print(f'Final OOF RMSE Score for all feature models: {np.mean(all_oof_score)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a924b8ec",
   "metadata": {
    "papermill": {
     "duration": 2.141024,
     "end_time": "2022-01-11T02:24:12.109849",
     "exception": false,
     "start_time": "2022-01-11T02:24:09.968825",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <span class=\"title-section w3-xxlarge\" id=\"ensemble\"> Ensemble Submission to Kaggle üá∞</span>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36974e80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-11T02:24:16.788246Z",
     "iopub.status.busy": "2022-01-11T02:24:16.787225Z",
     "iopub.status.idle": "2022-01-11T02:24:16.800833Z",
     "shell.execute_reply": "2022-01-11T02:24:16.801439Z",
     "shell.execute_reply.started": "2022-01-10T15:54:25.784997Z"
    },
    "papermill": {
     "duration": 2.219823,
     "end_time": "2022-01-11T02:24:16.801591",
     "exception": false,
     "start_time": "2022-01-11T02:24:14.581768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "final_final_preds_catb = np.mean(np.concatenate([np.expand_dims(pred.flatten(), axis = 0) for pred in catb_preds], axis = 0), axis=0)\n",
    "final_final_preds_torch = np.mean(np.concatenate([np.expand_dims(pred.flatten(), axis = 0) for pred in all_preds], axis = 0), axis=0)\n",
    "# final_final_preds_nets = np.mean(np.concatenate([np.expand_dims(pred.flatten(), axis = 0) for pred in first_stage_preds], axis = 0), axis=0)\n",
    "\n",
    "submission_df = pd.read_csv(f'{DATA_DIR}sample_submission.csv')\n",
    "# submission_df['Pawpularity'] = (final_final_preds_torch * 0.8) + (final_final_preds_catb * 0.12) + (final_final_preds_nets * 0.08)\n",
    "submission_df['Pawpularity'] = (final_final_preds_torch * 0.9) + (final_final_preds_catb * 0.1)\n",
    "submission_df.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11593.676957,
   "end_time": "2022-01-11T02:24:22.222379",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-01-10T23:11:08.545422",
   "version": "2.3.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1cb47d9d45f14c8596ec6d5ac6993212": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3320be3e6f6b4555be5dad1c578296f3",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_efe538e2cfcd424cb0c6b854b75a8297",
       "value": ""
      }
     },
     "3166a50e19af4faa9e566e933066748d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4270ad32adfc446195ea721b775fc4f9",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_e7a1a8c836804564ac408e38103708c6",
       "value": " 0/0 [00:00&lt;?, ?it/s]"
      }
     },
     "3320be3e6f6b4555be5dad1c578296f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3371d2497cc648a1b3a5feb2fb8decf1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4d5bf79ba346402e9f8151864d77797a",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f9a51a92133742f089fd6f4b62210dbe",
       "value": 0.0
      }
     },
     "4270ad32adfc446195ea721b775fc4f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4d5bf79ba346402e9f8151864d77797a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "b9651364868e4f7b9d8afaa6652ea5e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7a1a8c836804564ac408e38103708c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "efe538e2cfcd424cb0c6b854b75a8297": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f9a51a92133742f089fd6f4b62210dbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "fe485b7cb2744b018c44c7d786e7dd72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1cb47d9d45f14c8596ec6d5ac6993212",
        "IPY_MODEL_3371d2497cc648a1b3a5feb2fb8decf1",
        "IPY_MODEL_3166a50e19af4faa9e566e933066748d"
       ],
       "layout": "IPY_MODEL_b9651364868e4f7b9d8afaa6652ea5e9"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
