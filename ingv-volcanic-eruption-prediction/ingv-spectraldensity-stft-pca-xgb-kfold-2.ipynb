{"cells":[{"metadata":{},"cell_type":"markdown","source":"In this notebook, I am using the global statistical values (mean, max, min etc), spectral density of the sensor signals and shorttime fourier transforms as the features. \nI am using the principal components (first 21 components that explain upto 99.9% of the variance)of the spectral density features to reduce overfitting. \nFinally I am training an XGB model with five fold validation(Ordering the data in increasing order of time to eruption so as to ensure a similar distribution in each of the folds)"},{"metadata":{},"cell_type":"markdown","source":"I referred to the following notebooks to obtain ideas and some code.Thanks to the respective authors.\n1. https://www.kaggle.com/soheild91/ingv-nn-xgb-baseline (Getting started)\n2. https://www.kaggle.com/amanooo/ingv-volcanic-basic-solution-stft (STFT features)\n3. https://www.kaggle.com/kylesnyder/ingv-spectral-density-w-randomforest (Spectral density features)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\n\nimport numpy as np\nimport pandas as pd\n\nimport scipy\nfrom scipy import signal\nimport random\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\n\nimport xgboost","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Spectral density features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #spectral density along with statistical features\ninput_df = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train.csv')\ndata = np.empty((input_df.shape[0],1410))#129 features per sensor 10 sensors =1290 + 120 stat ftrs(12*10)\n#data = np.empty((input_df.shape[0],1290))\ntime = np.empty((input_df.shape[0],1))\n\nfor i,segment in enumerate(tqdm(input_df['segment_id'])):\n    temp = pd.read_csv(f'../input/predict-volcanic-eruptions-ingv-oe/train/{segment}.csv').fillna(0)\n    temp_arr = np.empty((0,))\n    for col in temp.columns:\n        freq,psd =signal.welch(temp[col],100)\n        temp_arr= np.concatenate((temp_arr,psd))  \n    temp_arr = np.concatenate((temp_arr,temp.abs().mean().to_numpy(),\n                               temp.std().to_numpy(),\n                               temp.mean().to_numpy(),\n                               temp.var().to_numpy(),\n                               temp.min().to_numpy(),\n                               temp.max().to_numpy(),\n                               temp.median().to_numpy(),\n                               temp.quantile([0.1,0.25,0.5,0.75,0.9]).to_numpy().reshape(1,-1)[0]))\n    temp_arr = temp_arr.reshape((1,-1))\n    data[i,:] = temp_arr\n    time[i,0] = input_df.loc[i,'time_to_eruption']","execution_count":3,"outputs":[{"output_type":"stream","text":"  0%|          | 20/4431 [00:03<13:21,  5.50it/s]\n","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-62992dded755>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segment_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../input/predict-volcanic-eruptions-ingv-oe/train/{segment}.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtemp_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1194\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2154\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2155\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2156\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df=pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv',nrows=5)\ndata_test=np.empty((sample_submission_df.shape[0],1410))\n#data_test=np.empty((sample_submission_df.shape[0],1290))\n\nfor i,segment in enumerate(tqdm(sample_submission_df['segment_id'])):\n    temp=pd.read_csv(f'../input/predict-volcanic-eruptions-ingv-oe/test/{segment}.csv').fillna(0)\n    temp_arr = np.empty((0,))\n    for col in temp.columns:\n        freq,psd =signal.welch(temp[col],100)\n        temp_arr= np.concatenate((temp_arr,psd))\n    temp_arr = np.concatenate((temp_arr,temp.abs().mean().to_numpy(),\n                                 temp.std().to_numpy(),\n                                 temp.mean().to_numpy(),\n                                 temp.var().to_numpy(),\n                                 temp.min().to_numpy(),\n                                 temp.max().to_numpy(),\n                                 temp.median().to_numpy(),\n                                 temp.quantile([0.1,0.25,0.5,0.75,0.9]).to_numpy().reshape(1,-1)[0]))\n    temp_arr = temp_arr.reshape((1,-1))\n    data_test[i,:] = temp_arr","execution_count":9,"outputs":[{"output_type":"stream","text":"100%|██████████| 5/5 [00:00<00:00,  5.53it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_sd = pd.DataFrame(data)\ntest_df_sd = pd.DataFrame(data_test)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('train_df_sd.pkl', 'wb') as f:\n    pickle.dump(train_df_sd, f)\n    \nwith open('test_df_sd.pkl', 'wb') as f:\n    pickle.dump(test_df_sd, f)","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# STFT Features"},{"metadata":{"trusted":true},"cell_type":"code","source":"#STFT(Short Time Fourier Transform) Specifications\nfs = 100                # sampling frequency \n#N = len(feature_df)     # data size\nN = 60000\nn = 256                 # FFT segment size\nmax_f = 20              # ～20Hz\n\ndelta_f = fs / n        # 0.39Hz\ndelta_t = n / fs / 2    # 1.28s","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #path ../input/predict-volcanic-eruptions-ingv-oe/test\ndef make_features(input_df, path):\n    feature_set = []\n    for i,segment_id in enumerate(input_df['segment_id']):\n        temp=pd.read_csv(f'{path}/{segment_id}.csv').fillna(0)\n        segment = [segment_id]\n        for sensor in temp.columns:\n            x = temp[sensor][:N]\n            if x.isna().sum() > 1000:     ##########\n                segment += ([np.NaN] * 10)\n                continue\n            f, t, Z = scipy.signal.stft(x.fillna(0), fs = fs, window = 'hann', nperseg = n)\n            f = f[:round(max_f/delta_f)+1]\n            Z = np.abs(Z[:round(max_f/delta_f)+1]).T    # ～max_f, row:time,col:freq\n\n            th = Z.mean() * 1     ##########\n            Z_pow = Z.copy()\n            Z_pow[Z < th] = 0\n            Z_num = Z_pow.copy()\n            Z_num[Z >= th] = 1\n\n            Z_pow_sum = Z_pow.sum(axis = 0)\n            Z_num_sum = Z_num.sum(axis = 0)\n\n            A_pow = Z_pow_sum[round(10/delta_f):].sum()\n            A_num = Z_num_sum[round(10/delta_f):].sum()\n            BH_pow = Z_pow_sum[round(5/delta_f):round(8/delta_f)].sum()\n            BH_num = Z_num_sum[round(5/delta_f):round(8/delta_f)].sum()\n            BL_pow = Z_pow_sum[round(1.5/delta_f):round(2.5/delta_f)].sum()\n            BL_num = Z_num_sum[round(1.5/delta_f):round(2.5/delta_f)].sum()\n            C_pow = Z_pow_sum[round(0.6/delta_f):round(1.2/delta_f)].sum()\n            C_num = Z_num_sum[round(0.6/delta_f):round(1.2/delta_f)].sum()\n            D_pow = Z_pow_sum[round(2/delta_f):round(4/delta_f)].sum()\n            D_num = Z_num_sum[round(2/delta_f):round(4/delta_f)].sum()\n            segment += [A_pow, A_num, BH_pow, BH_num, BL_pow, BL_num, C_pow, C_num, D_pow, D_num]\n\n        feature_set.append(segment)\n\n    cols = ['segment_id']\n    for i in range(10):\n        for j in ['A_pow', 'A_num','BH_pow', 'BH_num','BL_pow', 'BL_num','C_pow', 'C_num','D_pow', 'D_num']:\n            cols += [f's{i+1}_{j}']\n    feature_df = pd.DataFrame(feature_set, columns = cols)\n    feature_df['segment_id'] = feature_df['segment_id'].astype('int')\n    return feature_df","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_df = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train.csv')\npath_train = \"../input/predict-volcanic-eruptions-ingv-oe/train\"\ntrain_df_stft = make_features(input_df,path_train)\ntrain_df_stft = pd.merge(input_df, train_df_stft, on = 'segment_id')\n#test\nsample_submission_df=pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\npath_test = \"../input/predict-volcanic-eruptions-ingv-oe/test\"\ntest_df_stft = make_features(sample_submission_df,path_test)\n","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('train_df_stft.pkl', 'wb') as f:\n    pickle.dump(train_df_stft, f)\n    \nwith open('test_df_stft.pkl', 'wb') as f:\n    pickle.dump(test_df_stft, f)","execution_count":27,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#↑のデータは全て事前に作っておいて、ここからスタートしてください****"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_stft = pd.read_pickle('../input/ingv-data/train_df_stft.pkl')\ntime = train_df_stft['time_to_eruption']\ntrain_df_stft = train_df_stft.drop(['segment_id', 'time_to_eruption'], axis=1)\n\n\ntest_df_stft = pd.read_pickle('../input/ingv-data/test_df_stft.pkl')\ntest_df_stft = test_df_stft.drop(['segment_id'],axis = 1)\n\ntrain_df_sd = pd.read_pickle('../input/ingv-data/train_df_sd.pkl')\ntest_df_sd = pd.read_pickle('../input/ingv-data/test_df_sd.pkl')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_sd.head()","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"           0             1             2             3             4     \\\n0  11149.979226  56815.692310  30880.323186  20313.208263  14055.766676   \n1   8639.155232  53938.159167  96147.057293  91814.140016  61594.287618   \n2   5242.809579  28238.359471  23425.556184  16529.638069  14384.604091   \n3   1634.848410   9772.758686  21485.432441  23905.653426  20394.542189   \n4   4283.858707  21653.494775  11665.881656  16214.825532  18377.954876   \n\n           5             6             7             8            9     ...  \\\n0  10393.135986   6630.271440   4491.056525   3441.780685  2203.027738  ...   \n1  35624.272697  31678.645122  24611.483525  14252.425074  8810.443189  ...   \n2  12603.827779   6918.195897   4844.703328   4251.058629  3040.150635  ...   \n3  12118.667708   8169.856366   5419.216459   3400.171831  2440.077550  ...   \n4  15524.832550   9576.657305  10686.961788   8907.870405  4364.788805  ...   \n\n    1400   1401   1402   1403   1404   1405   1406   1407   1408    1409  \n0  383.0  799.0  368.0  493.0    0.0  888.0  676.0  798.0  471.0   767.0  \n1  528.0  782.0  433.0  452.0  318.0  422.0  569.0  456.0  496.0  1146.0  \n2  307.0  934.0  265.0  363.0    0.0  579.0  443.0  448.0  317.0   624.0  \n3  273.0  507.0  236.0  296.0  198.0  390.0  357.0  410.0  284.0   652.0  \n4  329.0    0.0  290.0  330.0  232.0  582.0  398.0  528.0  331.0   698.0  \n\n[5 rows x 1410 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>1400</th>\n      <th>1401</th>\n      <th>1402</th>\n      <th>1403</th>\n      <th>1404</th>\n      <th>1405</th>\n      <th>1406</th>\n      <th>1407</th>\n      <th>1408</th>\n      <th>1409</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>11149.979226</td>\n      <td>56815.692310</td>\n      <td>30880.323186</td>\n      <td>20313.208263</td>\n      <td>14055.766676</td>\n      <td>10393.135986</td>\n      <td>6630.271440</td>\n      <td>4491.056525</td>\n      <td>3441.780685</td>\n      <td>2203.027738</td>\n      <td>...</td>\n      <td>383.0</td>\n      <td>799.0</td>\n      <td>368.0</td>\n      <td>493.0</td>\n      <td>0.0</td>\n      <td>888.0</td>\n      <td>676.0</td>\n      <td>798.0</td>\n      <td>471.0</td>\n      <td>767.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8639.155232</td>\n      <td>53938.159167</td>\n      <td>96147.057293</td>\n      <td>91814.140016</td>\n      <td>61594.287618</td>\n      <td>35624.272697</td>\n      <td>31678.645122</td>\n      <td>24611.483525</td>\n      <td>14252.425074</td>\n      <td>8810.443189</td>\n      <td>...</td>\n      <td>528.0</td>\n      <td>782.0</td>\n      <td>433.0</td>\n      <td>452.0</td>\n      <td>318.0</td>\n      <td>422.0</td>\n      <td>569.0</td>\n      <td>456.0</td>\n      <td>496.0</td>\n      <td>1146.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5242.809579</td>\n      <td>28238.359471</td>\n      <td>23425.556184</td>\n      <td>16529.638069</td>\n      <td>14384.604091</td>\n      <td>12603.827779</td>\n      <td>6918.195897</td>\n      <td>4844.703328</td>\n      <td>4251.058629</td>\n      <td>3040.150635</td>\n      <td>...</td>\n      <td>307.0</td>\n      <td>934.0</td>\n      <td>265.0</td>\n      <td>363.0</td>\n      <td>0.0</td>\n      <td>579.0</td>\n      <td>443.0</td>\n      <td>448.0</td>\n      <td>317.0</td>\n      <td>624.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1634.848410</td>\n      <td>9772.758686</td>\n      <td>21485.432441</td>\n      <td>23905.653426</td>\n      <td>20394.542189</td>\n      <td>12118.667708</td>\n      <td>8169.856366</td>\n      <td>5419.216459</td>\n      <td>3400.171831</td>\n      <td>2440.077550</td>\n      <td>...</td>\n      <td>273.0</td>\n      <td>507.0</td>\n      <td>236.0</td>\n      <td>296.0</td>\n      <td>198.0</td>\n      <td>390.0</td>\n      <td>357.0</td>\n      <td>410.0</td>\n      <td>284.0</td>\n      <td>652.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4283.858707</td>\n      <td>21653.494775</td>\n      <td>11665.881656</td>\n      <td>16214.825532</td>\n      <td>18377.954876</td>\n      <td>15524.832550</td>\n      <td>9576.657305</td>\n      <td>10686.961788</td>\n      <td>8907.870405</td>\n      <td>4364.788805</td>\n      <td>...</td>\n      <td>329.0</td>\n      <td>0.0</td>\n      <td>290.0</td>\n      <td>330.0</td>\n      <td>232.0</td>\n      <td>582.0</td>\n      <td>398.0</td>\n      <td>528.0</td>\n      <td>331.0</td>\n      <td>698.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1410 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.decomposition import PCA\npca = PCA(n_components=0.999,svd_solver=\"full\") #all components that explain upto 99.9% of the variance\ntrain_pca = pca.fit_transform(train_df_sd)\ntrain_pca = pd.DataFrame(train_pca)\ntest_pca = pca.transform(test_df_sd)\ntest_pca = pd.DataFrame(test_pca)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.concat([train_df_stft,train_pca] ,axis=1)\ntest_df = pd.concat([test_df_stft,test_pca],axis = 1)","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGB with stratified k fold "},{"metadata":{"trusted":true},"cell_type":"code","source":"input_df = pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/train.csv')\ninput_df=input_df.copy()\ninput_df = input_df.sort_values(\"time_to_eruption\")\nfold_list = [1,2,3,4,5]\nfolds = []\nfor i in range(int((input_df.shape[0]-1)/5)):\n    random.shuffle(fold_list)\n    folds.extend(fold_list)\nfolds = folds + [1] #adding a remaining solitary record to fold 1\ninput_df['fold'] = folds\ninput_df.head(20)","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"      segment_id  time_to_eruption  fold\n590    601524801              6250     5\n2709  1658693785             25730     2\n1145  1957235969             26929     4\n413    442994108             28696     3\n1724  1626437563             40492     1\n3942   775594946             52074     2\n1291   765529516             58525     4\n3433    17116587             84289     5\n4150  1534910358             84390     1\n1126   827803506             87064     3\n1230  1593620672             92212     1\n3112  1145919143             96306     4\n2136   973420892             98516     2\n3339  1466017476            133215     3\n1747  1494708577            136062     5\n3196   339032264            142431     2\n1632   153007144            166202     4\n1545   588264306            171324     5\n603    372844561            179818     1\n4108   503441159            196451     3","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>segment_id</th>\n      <th>time_to_eruption</th>\n      <th>fold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>590</th>\n      <td>601524801</td>\n      <td>6250</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2709</th>\n      <td>1658693785</td>\n      <td>25730</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1145</th>\n      <td>1957235969</td>\n      <td>26929</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>413</th>\n      <td>442994108</td>\n      <td>28696</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1724</th>\n      <td>1626437563</td>\n      <td>40492</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3942</th>\n      <td>775594946</td>\n      <td>52074</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1291</th>\n      <td>765529516</td>\n      <td>58525</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3433</th>\n      <td>17116587</td>\n      <td>84289</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4150</th>\n      <td>1534910358</td>\n      <td>84390</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1126</th>\n      <td>827803506</td>\n      <td>87064</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1230</th>\n      <td>1593620672</td>\n      <td>92212</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3112</th>\n      <td>1145919143</td>\n      <td>96306</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2136</th>\n      <td>973420892</td>\n      <td>98516</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3339</th>\n      <td>1466017476</td>\n      <td>133215</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1747</th>\n      <td>1494708577</td>\n      <td>136062</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3196</th>\n      <td>339032264</td>\n      <td>142431</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1632</th>\n      <td>153007144</td>\n      <td>166202</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1545</th>\n      <td>588264306</td>\n      <td>171324</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>603</th>\n      <td>372844561</td>\n      <td>179818</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4108</th>\n      <td>503441159</td>\n      <td>196451</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x, val_x, train_y, val_y=train_test_split(train_df, time,random_state=42, test_size=0.2,shuffle=True)","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\nimport optuna\nfrom optuna.samplers import TPESampler\n\n#LGBMRegressor + optuna\nsampler=TPESampler(seed=42)\n\ndef create_model_xgb(trial):\n    n_estimators=trial.suggest_int(\"n_estimators\",50,300)\n    max_depth=trial.suggest_int(\"max_depth\",3,15)\n    learning_rate=trial.suggest_uniform(\"learning_rate\",0.0001,0.99)\n    gamma=trial.suggest_uniform('min_data_in_leaf',0,1)\n    model=xgb.XGBRegressor(\n    n_estimators=n_estimators,\n    max_depth=max_depth,\n    learning_rate=learning_rate,\n    gamma=gamma,\n    random_state=42)\n    \n    return model\n\n\n#目的関数\ndef objective_xgb(trial):\n    model1=create_model_xgb(trial)\n    model1.fit(train_x,train_y)\n    preds=model1.predict(val_x)\n    score=mean_absolute_error(val_y,preds)\n    return score\n\n\nstudy1=optuna.create_study(direction=\"minimize\", sampler=sampler)\noptuna.logging.disable_default_handler()#don't show log\nstudy1.optimize(objective_xgb, n_trials=100)\n\n#最適解\nprint(study1.best_params)\nprint(study1.best_value)\nprint(study1.best_trial)","execution_count":36,"outputs":[{"output_type":"stream","text":"[I 2021-01-02 07:19:52,395] A new study created in memory with name: no-name-fa9cd07e-599b-4010-b0e4-3f67a1fb975c\n","name":"stderr"},{"output_type":"stream","text":"{'n_estimators': 170, 'max_depth': 12, 'learning_rate': 0.04915558807197376, 'min_data_in_leaf': 0.5416341991105386}\n2353906.032764938\nFrozenTrial(number=98, value=2353906.032764938, datetime_start=datetime.datetime(2021, 1, 2, 7, 31, 54, 45970), datetime_complete=datetime.datetime(2021, 1, 2, 7, 32, 3, 468482), params={'n_estimators': 170, 'max_depth': 12, 'learning_rate': 0.04915558807197376, 'min_data_in_leaf': 0.5416341991105386}, distributions={'n_estimators': IntUniformDistribution(high=300, low=50, step=1), 'max_depth': IntUniformDistribution(high=15, low=3, step=1), 'learning_rate': UniformDistribution(high=0.99, low=0.0001), 'min_data_in_leaf': UniformDistribution(high=1, low=0)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=98, state=TrialState.COMPLETE)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"params=study1.best_params\nparams","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"{'n_estimators': 170,\n 'max_depth': 12,\n 'learning_rate': 0.04915558807197376,\n 'min_data_in_leaf': 0.5416341991105386}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.zeros(len(test_df))\nfor fold in tqdm(range(1,6)):\n    train_index_list = input_df[input_df['fold'] != fold].index\n    test_index_list = input_df[input_df['fold'] == fold].index\n\n    X_train = train_df.iloc[train_index_list]\n    y_train = time[train_index_list]\n    X_val = train_df.iloc[test_index_list]\n    y_val = time[test_index_list]\n\n    model = xgboost.XGBRegressor(**params)\n    \n    eval_set = [(X_val, y_val)]\n    \n    model.fit(X_train, y_train,early_stopping_rounds=10,eval_metric='mae', eval_set=eval_set, verbose=False)\n    #print(model.evals_result()['validation_0']['mae'][-5:])\n    predictions += model.predict(test_df)\npredictions = predictions/5","execution_count":41,"outputs":[{"output_type":"stream","text":"\r  0%|          | 0/5 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"[07:39:15] WARNING: ../src/learner.cc:516: \nParameters: { min_data_in_leaf } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n","name":"stdout"},{"output_type":"stream","text":"\r 20%|██        | 1/5 [00:09<00:38,  9.72s/it]","name":"stderr"},{"output_type":"stream","text":"[07:39:24] WARNING: ../src/learner.cc:516: \nParameters: { min_data_in_leaf } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n","name":"stdout"},{"output_type":"stream","text":"\r 40%|████      | 2/5 [00:19<00:29,  9.68s/it]","name":"stderr"},{"output_type":"stream","text":"[07:39:34] WARNING: ../src/learner.cc:516: \nParameters: { min_data_in_leaf } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n","name":"stdout"},{"output_type":"stream","text":"\r 60%|██████    | 3/5 [00:29<00:19,  9.88s/it]","name":"stderr"},{"output_type":"stream","text":"[07:39:44] WARNING: ../src/learner.cc:516: \nParameters: { min_data_in_leaf } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n","name":"stdout"},{"output_type":"stream","text":"\r 80%|████████  | 4/5 [00:39<00:09,  9.75s/it]","name":"stderr"},{"output_type":"stream","text":"[07:39:54] WARNING: ../src/learner.cc:516: \nParameters: { min_data_in_leaf } might not be used.\n\n  This may not be accurate due to some parameters are only used in language bindings but\n  passed down to XGBoost core.  Or some parameters are not used but slip through this\n  verification. Please open an issue if you find above cases.\n\n\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 5/5 [00:48<00:00,  9.70s/it]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_df1=pd.read_csv('../input/predict-volcanic-eruptions-ingv-oe/sample_submission.csv')\nsample_submission_df1['time_to_eruption']=predictions\nsample_submission_df1.to_csv('xgb_5fldst_ft_sdpca_stft2.csv',index=False)","execution_count":31,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat":4,"nbformat_minor":4}